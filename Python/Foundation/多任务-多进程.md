[TOC]

# 多进程

```
Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

子进程永远返回0，而父进程返回子进程的ID。
这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。

有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。

在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。
```

## 创建子进程

```python
import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
```

结果

```
Process (876) start...
I (876) just created a child process (877).
I am child process (877) and my parent is 876.
```

## multiprocessing

```
multiprocessing模块就是跨平台版本的多进程管理模块，可以实现多进程的程序设计
支持子进程、通信和数据共享，提供多种形式的同步机制及Process、Queue、Pipe、Lock等组件
同时支持本地并发和远程并发，有效避免了全局解释器锁(GIL)问题，可以更有效地利用CPU资源，尤其适用于多核或多CPU环境
```

进程创建于管理模块

| 组件        | 描述                                                         |
| ----------- | ------------------------------------------------------------ |
| process     | 用于创建子进程，可以实现多进程的创建、启动、关闭等操作       |
| pool        | 用于创建管理进程池，当子进程非常多且需要控制子进程数量时使用 |
| manager     | 通常与pool一同使用，用于资源共享                             |
| pipe        | 用于进程间的管道通信                                         |
| queue       | 用于进程通信                                                 |
| value,array | 用于进程通信，资源共享                                       |

子进程同步模块

| 组件      | 描述                         |
| --------- | ---------------------------- |
| condition | 条件变量                     |
| event     | 用来实现进程间的同步通信     |
| lock      | 锁                           |
| rlock     | 多重锁                       |
| semaphore | 用来控制对共享资源的访问数量 |

### Process

```
multiprocessing模块提供了一个Process类来代表一个进程对象，这个对象可以理解为是一个独立的进程，可以执行另外的事情

# Process创建的实例对象
Process([group [, target [, name [, args [, kwargs]]]]])
target：如果传递了函数的引用，可以任务这个子进程就执行这里的代码
args：给target指定的函数传递的参数，以元组的方式传递
kwargs：给target指定的函数传递命名参数
name：给进程设定一个名字，可以不设定
group：指定进程组，大多数情况下用不到
```

实例方法

| 方法              | 说明                                                         |
| ----------------- | ------------------------------------------------------------ |
| `is_alive`        | 返回进程是否在运行                                           |
| `join([timeout])` | 阻塞当前上下文环境的进程，直到调用此方法的进程终止或到达指定的timeout(可选参数) |
| `start()`         | 启动子进程实例(创建子进程)，进程准备就绪，等待CPU调度        |
| `run()`           | `start()`调用run方法，如果实例进程时未指定传入target,则star默认执行run方法 |
| ` terminate()`    | 不管任务是否完成，立即终止子进程                             |

属性

| 属性     | 说明                                                         |
| -------- | ------------------------------------------------------------ |
| daemon   | 与线程setDeamon的功能一样(将父进程设置为守护进程，当父进程结束时，子进程也结束) |
| exitcode | 进程在运行时为None,如果为-N,则表示被信号N结束                |
| name     | 当前进程的别名，默认为Process-N，N为从1开始递增的整数        |
| pid      | 当前进程的进程号                                             |

- 样例

> 启动一个子进程并等其结束

```python
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
	# os.getpid():获取当前程序进程号
    print('Parent process %s.' % os.getpid())
    # 传入子进程函数名和参数 
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    # 开启子进程
    p.start()
    # 等待子进程结束
    p.join()
    print('Child process end.')
```

结果

```
Parent process 928.
Child Process will start.
Run child process test (929)...
Process end.
```

> 创建多个进程

```python
import multiprocessing
import time

def process1(interval):
	while True:
		print('process1 is runing')
		time.sleep(interval)

def process2(interval):
	while True:
		print('process2 is runing')
		time.sleep(interval)
		
if __name__ == '__main__':
	p1 = multiprocessing.Process(target = process1, args = (2,))
	p2 = multiprocessing.Process(target = process2, args = (2,))
	p1.start();
	p2.start();
	
	while True:
		for p in multiprocessing.active_children():
			print('child Process:' + p.name + '\t,id:' + str(p.id) + 'is alive')
		print('main process is running')
        time.sleep(2)
```

> 将进程定义为类

```python
import multiprocessing
import time

class ChildProcess(multiprocessing.Process):
	def __init__(self, interval):
        multiprocessing.Process.__init__(self)
        self.interval = interval
        
    def run(self):
        while True:
            print('ChildProcess is runing')
            time.sleep(self.interval)
            
 
if __name__ == '__main__':
    p = ChildProcess(2)
    p.start()
    while True:
        print('MainProcess is running')
        time.sleep(2)
```

## 进程通信

当多个进程需要访问共享资源时，为了避免冲突multiprocessing模块提供了多种机制实现进程间同步

### Lock

锁机制通过对共享资源上锁的方式避免多个进程的访问冲突

```python
import multiprocessing
import sys

def process1(lock, f):
    with lock:
        fs = open(f, 'a+')
        times = 10
        while times > 0:
            fs.write('process1 write\n')
            times -= 1
        fs.close()
        
def process2(lock, f):
    # 上锁
    lock.acquire()
    try:
        fs = open(f, 'a+')
        times = 10
        while times > 0:
            fs.write('process2 write\n')
            tiems -= 1
        fs.close()
    finally:
        // 解锁
        lock.release()
        
        
if __name__ == '__main__':
    # 创建锁
    lock = multiprocessing.Lock()
    f = 'share.txt'
    p1 = multiprocessing.Process(target = process1, args=(lock, f))
    p2 = multiprocessing.Process(target = process2, args=(lock, f))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
```

### RLock

RLock是Lock的递归版。`lock.aquire()`是请求锁，当前的锁为锁定状态时，`lock.acquire()`会阻塞等待锁释放。若写了两个`lock.aquire()`会产生死锁，则第二个`lock.aquire()`会永远等待在那里

使用RLock则不会出现这种情况，RLock支持在同一资源上多个锁，上多少把锁，就得释放多少次。

### Event

```python
from multiprocessing import Process, Event

def f(e, i):
    if e.is_set():
        e.wait()
        print('hello word', i)
        e.clear()
    else:
        e.set()
        
        
if __name__ == '__main__':
    e = Event()
    for num in range(10):
        Process(target=f, args=(e, num)).start()
```

### Semaphore

semaphore有信号量的意思，与Lock有些类似。Semaphore可以指定允许访问资源的进程数量。通俗来讲就是，该资源有多个门，每个门对应一把锁。一个进程访问了该资源，锁了门，还有其他门可以使用。如果所有的门都被锁了，那么新的进程就必须等待现有进程推出并释放锁后才可以访问

```python
import multiprocessing
import time

def process1():
	s.acquire()
	print('process1 acquire and it will sleep 5s')
    time.sleep(5)
    print('process1 release');
    s.release()
    
def process2():
    s.acquire()
    print('process2 acquire and it will sleep 5s')
    time.sleep(5)
    print('process2 release')
    s.release()
    
def process3():
    print('process3 try to start')
    s.acquire()
    print('process3 acquire and it will sleep 5s')
    time.sleep(5)
    print('process release')
    s.release()
    
if __name__ == '__main__':
    # 限制为最多两个进程同时访问
    s = multiprocessing.Semaphore(2)
    p1 = multiprocessing.Process(target = process1)
    p2 = multiprocessing.Process(target = process2)
    p3 = multiprocessing.Process(target = process3)
    # 依次启动3个进程，当前两个进程未推出时，进程3尝试访问失败，当进程1退出后，进程3才获得权限
    p1.start()
    time.sleep(1)
    p2.start()
    time.sleep(1)
    p3.start()
    time.sleep(1)
```

### Queue

操作系统提供了很多机制来实现进程间的通信。Python的`multiprocessing`模块包装了底层的机制，提供了`Queue`、`Pipes`等多种方式来交换数据。Pipe是管道，Queue是队列

Queue对象的方法

```
初始化Queue()对象时（例如：q=Queue()），若括号中没有指定最大可接收的消息数量，或数量为负值，那么就代表可接受的消息数量没有上限（直到内存的尽头）；

Queue.qsize()：返回当前队列包含的消息数量；

Queue.empty()：如果队列为空，返回True，反之False ；

Queue.full()：如果队列满了，返回True,反之False；

Queue.get([block[, timeout]])：获取队列中的一条消息，然后将其从列队中移除，block默认值为True；
1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出"Queue.Empty"异常；
2）如果block值为False，消息列队如果为空，则会立刻抛出"Queue.Empty"异常；

Queue.get_nowait()：相当Queue.get(False)；

Queue.put(item,[block[, timeout]])：将item消息写入队列，block默认值为True；
1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出"Queue.Full"异常；
2）如果block值为False，消息列队如果没有空间可写入，则会立刻抛出"Queue.Full"异常；

Queue.put_nowait(item)：相当Queue.put(item, False)；
```

代码

```python
# 以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

进程池中的Queue

如果要使用Pool创建进程，需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()

```python
# 修改import中的Queue为Manager
from multiprocessing import Manager,Pool
import os,time,random

def reader(q):
    print("reader启动(%s),父进程为(%s)" % (os.getpid(), os.getppid()))
    for i in range(q.qsize()):
        print("reader从Queue获取到消息：%s" % q.get(True))

def writer(q):
    print("writer启动(%s),父进程为(%s)" % (os.getpid(), os.getppid()))
    for i in "itcast":
        q.put(i)

if __name__=="__main__":
    print("(%s) start" % os.getpid())
    q = Manager().Queue()  # 使用Manager中的Queue
    po = Pool()
    po.apply_async(writer, (q,))

    time.sleep(1)  # 先让上面的任务向Queue存入数据，然后再让下面的任务开始从中取数据

    po.apply_async(reader, (q,))
    po.close()
    po.join()
    print("(%s) End" % os.getpid())
```

## Pool

如果要启动大量的子进程，可以用进程池的方式批量创建子进程

Pool对象提供了大量的方法支持并行操作

```python
apply(func[, args[, kwds]])
# 调用函数func，并传递参数args和kwds，同时阻塞当前进程直至函数返回，函数func只会在进程池中的一个工作进程中运行

apply_async(func[, args[, kwds[, callback[, error_callback]]]])
# apply()的变形，返回结果对象，可以通过结果对象的get()方法获取其中的结果，参数callback和error_callback都是单参数函数，当结果对象可用时会自动调用callback，该调用失败时会自动调用error_callback

map(func, iterable[, chunksize])
# 内置函数map()的并行版本，但只能接收一个可迭代对象作为参数，该方法会阻塞当前进程直至结果可用。该方法会把迭代对象iterable切分成多个块再作为独立的任务提交给进程池，块的大小可以通过参数chunksize(默认1)来设置

map_async(func, iterable[, chunksize[, callback[, error_callback]]])
# 与map()方法类似，但返回结果对象，需要使用结果对象的get()方法来获取其中的值

imap(func, iterable[, chunksize[, callback[, error_callback]]])
# map()方法的惰性求值版本，返回迭代器对象

imap_unordered(func, iterable[, chunksize])
# 与imap()方法类似，但不保证结果会按参数iterable中原来元素的先后顺序返回

starmap(func, iterable[, chunksize])
# 类似于map()方法，但要求参数iterable中的元素为得带对象并可解包为函数func的参数

starmap_async(func, iterable[, chunksize[, callback[, error_back]]])
# 方法starmap()和map_async的组合，返回结果对象

close()
# 不允许再向进程池提交任务，当所有已提交任务完成后工作进程会退出

terminate()
# 立即结束工作进程，当线程池对象被回收时会自动调用该方法

join()
# 等待工作进程退出，在此之前必须先调用close()或treminate()
```

并发计算二维数组每行的平均值

```python
from multiprocessing import Pool
from statistics import mean

def f(x):
    return mean(x)

if __name__ == '__main__':
    x = [list(range(10)), list(range(20,30)),
         list(range(50, 60)), list(range(80, 90))]
    with Pool(5) as p:  # 创建包含5个进程的进程池
        print(p.map(f,x)) # 并发运行
```

Pool对象的几种常用方法

```python
from multiprocessing import Pool
import time

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(processes=4) as pool:
        # 返回结果对象，可以通过get()方法获取其中的值
        result = pool.apply_async(f, (10,))
        print(result.get(timeout=1))
        # 直接返回结果列表
        print(pool.map(f, range(10)))
        # 返回迭代器对象
        it - pool.imap(f, range(10))
        print(next(it))
        print(next(it))
        pritn(it.next(timeout=1))
        #  进入睡眠状态10s
        result = pool.apply_async(time.sleep,(10,))
        # 下面的代码会引发超时异常
        print(result.get(timeout=3))
```

代码

```python
 from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    # 定义一个进程池，最大进程数4
    p = Pool(4)
    for i in range(5):
    	# Pool().apply_async(要调用的目标,(传递给目标的参数元祖,))
    	# 每次循环将会用空闲出来的子进程去调用目标
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()	# 关闭进程池，关闭后pool不再接收新的请求
    p.join()	# 等待pool中所有子进程执行完成，必须放在close语句之后
    print('All subprocesses done.')
```

## Manager

Manager对象提供了不同进程间共享数据的方式，甚至可以在网络上不同机器上运行的进程间共享数据。Manager对象控制一个拥有`list,dict,Lock,RLock,Semphore,BoundedSemaphore,Condition,Event,Barrier,Queue,Value,Array,Namespace`等对象的服务端进程，并且允许其他进程通过代理来操作这些对象

使用Manager对象实现进程间数据交换

```python
from multiprocessing import Process, Manager

def f(d, l, t):
    d['name'] = 'Li Lei'
    d['age'] = 18
    d['sex'] = 'Male'
    d['address'] = 'Yantai'
    l.reverse()
    t.value = 3

if __name__ == "__main__":
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))
        t = manager.Value('i', 0)
        p = Process(target=f, args=(d,l,t))
        p.start()
        p.join()
        for item in d.items():
            print(item)
        print(1)
        print(t.value)
```

使用Manager对象实现不同机器上的进程跨网络共享数据

```python
# 1. 编写multiprocessing_server.py,启动服务进程，创建可共享的队列对象

```



## 子进程输入输出

很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。

`subprocess`模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。

```
# 在Python代码中运行命令nslookup www.python.org,等同命令行直接运行
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

如果子进程还需要输入，则可以通过`communicate()`方法输入：

```
# 等同命令行输入set q=mx  python.org  exit
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
```

## 分布式进程

```
在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。

Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。
```

如果我们已经有一个通过`Queue`通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？

原有的`Queue`可以继续使用，但是，通过`managers`模块把`Queue`通过网络暴露出去，就可以让其他机器的进程访问`Queue`了。

1. 我们先看服务进程，服务进程负责启动`Queue`，把`Queue`注册到网络上，然后往`Queue`里面写入任务：

```
# task_master.py

import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)
# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()
# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()
# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)
    print('Result: %s' % r)
# 关闭:
manager.shutdown()
print('master exit.')
```

请注意，当我们在一台机器上写多进程程序时，创建的`Queue`可以直接拿来用，但是，在分布式多进程环境下，添加任务到`Queue`不可以直接对原始的`task_queue`进行操作，那样就绕过了`QueueManager`的封装，必须通过`manager.get_task_queue()`获得的`Queue`接口添加。

然后，在另一台机器上启动任务进程（本机上启动也可以）：

```python
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')
```

任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。

现在，可以试试分布式进程的工作效果了。先启动`task_master.py`服务进程：

```
python3 task_master.py
```

`task_master.py`进程发送完任务后，开始等待`result`队列的结果。现在启动`task_worker.py`进程：

```
python3 task_worker.py
```

`task_worker.py`进程结束，在`task_master.py`进程中会继续打印出结果

**注意：**

Queue对象存储在`task_master.py`进程中，而`Queue`之所以能通过网络访问，就是通过`QueueManager`实现的。由于`QueueManager`管理的不止一个`Queue`，所以，要给每个`Queue`的网络调用接口起个名字，比如`get_task_queue`。

`authkey`是为了保证两台机器正常通信，不被其他机器恶意干扰。如果`task_worker.py`的`authkey`和`task_master.py`的`authkey`不一致，肯定连接不上

Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件

## 多进程版文件复制器

```
import multiprocessing
import os
import time
import random


def copy_file(queue, file_name,source_folder_name,  dest_folder_name):
    """copy文件到指定的路径"""
    f_read = open(source_folder_name + "/" + file_name, "rb")
    f_write = open(dest_folder_name + "/" + file_name, "wb")
    while True:
        time.sleep(random.random())
        content = f_read.read(1024)
        if content:
            f_write.write(content)
        else:
            break
    f_read.close()
    f_write.close()

    # 发送已经拷贝完毕的文件名字
    queue.put(file_name)


def main():
    # 获取要复制的文件夹
    source_folder_name = input("请输入要复制文件夹名字:")

    # 整理目标文件夹
    dest_folder_name = source_folder_name + "[副本]"

    # 创建目标文件夹
    try:
        os.mkdir(dest_folder_name)
    except:
        pass  # 如果文件夹已经存在，那么创建会失败

    # 获取这个文件夹中所有的普通文件名
    file_names = os.listdir(source_folder_name)

    # 创建Queue
    queue = multiprocessing.Manager().Queue()

    # 创建进程池
    pool = multiprocessing.Pool(3)

    for file_name in file_names:
        # 向进程池中添加任务
        pool.apply_async(copy_file, args=(queue, file_name, source_folder_name, dest_folder_name))

    # 主进程显示进度
    pool.close()

    all_file_num = len(file_names)
    while True:
        file_name = queue.get()
        if file_name in file_names:
            file_names.remove(file_name)

        copy_rate = (all_file_num-len(file_names))*100/all_file_num
        print("\r%.2f...(%s)" % (copy_rate, file_name) + " "*50, end="")
        if copy_rate >= 100:
            break
    print()


if __name__ == "__main__":
    main()
```

# 