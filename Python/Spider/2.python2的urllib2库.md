# Python2的urllib2库

```
urllib2 是 Python2.7 自带的模块(不需要下载，导入即可使用)
urllib2 官方文档：https://docs.python.org/2/library/urllib2.html
urllib2 源码：https://hg.python.org/cpython/file/2.7/Lib/urllib2.py

在 python3 中，urllib2 被改为urllib.request
```

## 基本使用

### urlopen

```
# urllib2_urlopen.py

# 导入urllib2 库
import urllib2

# 向指定的url发送请求，并返回服务器响应的类文件对象
# 参数是url地址
response = urllib2.urlopen("http://www.baidu.com")

# 类文件对象支持 文件对象的操作方法，如read()方法读取文件全部内容，返回字符串
html = response.read()

# 打印字符串
print html
```

eg

```
import urllib2

def download(url, num_retries=2):
	print 'Downloading:', url
	try:
		html = urllib2.open(url).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			# 当出现5xx错误时，会重新再请求2次
			if hasattr(e, 'code') and 500 <= e.code < 600:
				return download(url, num_retries-1)
	return html
```

### Request

```
# urllib2_request.py

import urllib2

# url 作为Request()方法的参数，构造并返回一个Request对象
# 参数还可以是data（默认空）：提交的Form表单数据，同时 HTTP 请求方法将从默认的 "GET"方式 改为 "POST"方式。
# headers（默认空）：参数为字典类型，包含了需要发送的HTTP报头的键值对。
request = urllib2.Request("http://www.baidu.com")

# Request对象作为urlopen()方法的参数，发送给服务器并接收响应
response = urllib2.urlopen(request)

html = response.read()

print html
```

### User-Agent

urllib2默认的User-Agent头为：Python-urllib/x.y （x和y 是Python 主.次 版本号，例如 Python-urllib/2.7）

```
#urllib2_useragent.py

import urllib2

url = "http://www.itcast.cn"

# IE 9.0 的 User-Agent，包含在 user_agent里
user_agent = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"} 

#  url 连同 headers，一起构造Request请求，这个请求将附带 IE9.0 浏览器的User-Agent
request = urllib2.Request(url, headers = user_agent)

# 向服务器发送这个请求
response = urllib2.urlopen(request)

html = response.read()
print html
```

### 添加Header信息

在 HTTP Request 中加入特定的 Header，来构造一个完整的HTTP请求消息。

```
# 添加/修改一个特定的header
Request.add_header()  
# 查看已有的header
Request.get_header()
```

- 添加一个特定的header

```
# urllib2_headers.py

import urllib2

url = "http://www.itcast.cn"

#IE 9.0 的 User-Agent
user_agent = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"} 
request = urllib2.Request(url, headers = user_agent)

#也可以通过调用Request.add_header() 添加/修改一个特定的header
request.add_header("Connection", "keep-alive")

# 也可以通过调用Request.get_header()来查看header信息
# request.get_header(header_name="Connection")

response = urllib2.urlopen(request)

print response.code     #可以查看响应状态码
html = response.read()

print html
```

- 随机添加/修改User-Agent

```
# urllib2_add_headers.py

import urllib2
import random

url = "http://www.itcast.cn"

ua_list = [
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
    "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6"
]

user_agent = random.choice(ua_list)

request = urllib2.Request(url)

#也可以通过调用Request.add_header() 添加/修改一个特定的header
request.add_header("User-Agent", user_agent)

# get_header()的字符串参数，第一个字母大写，后面的全部小写
request.get_header("User-agent")

response = urllib2.urlopen(request)

html = response.read()
print html
```

## GET/POST

### URL编码转换

```
urllib 和 urllib2 都是接受URL请求的相关模块,但是
1.urllib 模块仅可以接受URL，不能创建 设置了headers 的Request 类实例；
2.urllib 提供 urlencode方法用来产生GET查询字符串，而 urllib2 则没有。

# 编码：将key:value键值对，转换成"key=value"
urllib.urlencode()

# 解码工作
urllib.unquote()
```

### GET

#### 直接对url进行获取

GET请求一般用于我们向服务器获取数据，我们用百度搜索`传智播客`：<https://www.baidu.com/s?wd=传智播客>，浏览器的url会跳转成:<https://www.baidu.com/s?wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2>

```
# urllib2_get.py

import urllib      #负责url编码处理
import urllib2

url = "http://www.baidu.com/s"
word = {"wd":"传智播客"}
# 转换成url编码格式（字符串）
# 1.中文转换，2.字典转换
word = urllib.urlencode(word) 
newurl = url + "?" + word    # url首个分隔符就是 ?

headers={ "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"}

request = urllib2.Request(newurl, headers=headers)

response = urllib2.urlopen(request)

print response.read()
```
#### 获取AJAX加载的内容

AJAX请求一般返回给网页的是JSON文件，只要对AJAX请求地址进行POST或GET，就能返回JSON数据了。

```
# demo1

url = "https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action=&"

headers={"User-Agent": "Mozilla...."}

# 变动的是这两个参数，从start开始往后显示limit个
formdata = {
    'start':'0',
    'limit':'10'
}
data = urllib.urlencode(formdata)

request = urllib2.Request(url + data, headers = headers)
response = urllib2.urlopen(request)

print response.read()


# demo2

url = "https://movie.douban.com/j/chart/top_list?"
headers={"User-Agent": "Mozilla...."}

# 处理所有参数
formdata = {
    'type':'11',
    'interval_id':'100:90',
    'action':'',
    'start':'0',
    'limit':'10'
}
data = urllib.urlencode(formdata)

request = urllib2.Request(url + data, headers = headers)
response = urllib2.urlopen(request)

print response.read()
```

### POST

Request请求对象的里有data参数，它就是用在POST里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。

```
# coding:utf-8
import urllib2
import urllib
import json


# 如果网页端被反爬：如js加密，js编码，不建议去破解，另辟蹊径
# app 手机端
def fanyi_post_baidu():
    # 1.url
    url = 'http://fanyi.baidu.com/basetrans'
    # 注意点：app的User-agent
    headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602."
            }

    # 2.拼接参数
    keyword = raw_input('请输入翻译的内容：')
    fromdata = {
            "query": keyword,
            "from": "zh",
            "to": "en",
            }

    # 3.生成请求对象 POST
    fromdata_str = urllib.urlencode(fromdata)
    request = urllib2.Request(url, data=fromdata_str, headers =
            headers)
 
    # 4.发送请求
    response = urllib2.urlopen(request)
    data = response.read()
    print data

    # 转码
    # 1.将json字符串 --> python对象 字典dict
    dict_data = json.loads(data)
    print dict_data
    # 2.提取结果：dict(trans)-->[0]-->dict[result]-->[0]-->[1]
    result = dict_data['trans'][0]['result'][0][1]

    # 5.输出翻译结果
    print result


if __name__ == '__main__':
    fanyi_post_baidu()
```

### 区别

```
GET方式是直接以链接形式访问，链接中包含了所有的参数，服务器端用Request.QueryString获取变量的值。如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。

POST则不会在网址上显示所有的参数，服务器端用Request.Form获取提交的数据，在Form提交的时候。但是HTML代码里如果不指定 method 属性，则默认为GET请求，Form中提交的数据将会附加在url之后，以?分开与url分开。

表单数据可以作为 URL 字段（method="get"）或者 HTTP POST （method="post"）的方式来发送。
```

### SSL证书验证

urllib2可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，我们需要单独处理SSL证书，让程序忽略SSL证书验证错误，即可正常访问。

```
# coding:utf-8
import urllib2
import ssl


def ssl_load_data():
    # 1.url
    url = 'https://www.12306.cn/mormhweb/'
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko"}

    # 2.request
    request = urllib2.Request(url, headers=headers)

    # 3.response
    # 告知系统，忽略证书即可
    context = ssl._create_unverified_context()
    response = urllib2.urlopen(request, context=context)
    data = response.read()

    # 4.写入本地
    with open('raw_info/03ssl.html', 'w') as f:
        f.write(data)


if __name__ == '__main__':
    ssl_load_data()
```

## Handler处理器和自定义Opener

```
opener是 urllib2.OpenerDirector 的实例，我们之前一直都在使用的urlopen，它是一个特殊的opener（也就是模块帮我们构建好的）。

但是基本的urlopen()方法不支持代理、Cookie等其他的 HTTP/HTTPS高级功能。所以要支持这些功能：
1.使用相关的 Handler处理器 来创建特定功能的处理器对象；
2.然后通过 urllib2.build_opener()方法使用这些处理器对象，创建自定义opener对象；
3.使用自定义的opener对象，调用open()方法发送请求。
4.注意：如果程序里所有的请求都使用自定义的opener，可以使用urllib2.install_opener() 将自定义的 opener 对象 定义为 全局opener，表示如果之后凡是调用urlopen，都将使用这个opener（根据自己的需求来选择）。
```

### 简单自定义HTTPHandler

```
# coding:utf-8
import urllib2


def custom_openner_handler():
    # 1.创建处理器对象,支持处理HTTP请求，同时开启Debug Log，debuglevel 值默认 0
    custom_hanlder = urllib2.HTTPHandler(debuglevel=1)
    # 2.根据处理器对象，生成openner对象
    openner = urllib2.build_opener(custom_hanlder)
    # 3.调用open请求数据
    response = openner.open('http://www.baidu.com')
    print response.read()


if __name__ == '__main__':
    custom_openner_handler()
```

### ProxyHandler

```
import urllib2
import random

proxy_list = [
    {"http" : "124.88.67.81:80"},
    {"http" : "124.88.67.81:80"},
    {"http" : "124.88.67.81:80"},
    {"http" : "124.88.67.81:80"},
    {"http" : "124.88.67.81:80"}
]

# 随机选择一个代理
proxy = random.choice(proxy_list)
# 使用选择的代理构建代理处理器对象
httpproxy_handler = urllib2.ProxyHandler(proxy)
# 创建自定义opener对象
opener = urllib2.build_opener(httpproxy_handler)
# 创建请求对象
request = urllib2.Request("http://www.baidu.com/")

# 1. 如果这么写，只有使用opener.open()方法发送请求才使用自定义的代理，而urlopen()则不使用自定义代理。
response = opener.open(request)

# 2. 如果这么写，就是将opener应用到全局，之后所有的，不管是opener.open()还是urlopen() 发送请求，都将使用自定义代理。
# urllib2.install_opener(opener)
# response = urlopen(request)
print response.read()
```

### ProxyBasicAuthHandler

`HTTPPasswordMgrWithDefaultRealm()`类将创建一个密码管理对象，用来保存 HTTP 请求相关的用户名和密码，主要应用两个场景：

1. 验证代理授权的用户名和密码 (`ProxyBasicAuthHandler()`)
2. 验证Web客户端的的用户名和密码 (`HTTPBasicAuthHandler()`)

如果我们使用之前的代码来使用私密代理，会报 HTTP 407 错误，表示代理没有通过身份验证。通过：

- `HTTPPasswordMgrWithDefaultRealm()`：来保存私密代理的用户密码
- `ProxyBasicAuthHandler()`：来处理代理的身份验证。

```
import urllib2
import urllib

# 私密代理授权的账户
user = "mr_mao_hacker"
# 私密代理授权的密码
passwd = "sffqry9r"
# 私密代理 IP
proxyserver = "61.158.163.130:16816"

# 1. 构建一个HTTP密码管理对象，用来保存需要处理的用户名和密码
#passwdmgr = urllib2.HTTPPasswordMgrWithDefaultRealm()

# 2. 添加账户信息，第一个参数realm是与远程服务器相关的域信息，一般没人管它都是写None，后面三个参数分别是 代理服务器、用户名、密码
#passwdmgr.add_password(None, proxyserver, user, passwd)

# 3. 构建一个代理基础用户名/密码验证的ProxyBasicAuthHandler处理器对象，参数是创建的密码管理对象
#   注意，这里不再使用普通ProxyHandler类了
#proxyauth_handler = urllib2.ProxyBasicAuthHandler(passwdmgr)


# BTW：
# 工作中常用下面的方式直接创建一个附带私密代理验证的处理器，使用更加简洁明了，并不需要上面3步的代码

# 1. 构建一个附带Auth验证的的ProxyHandler处理器类对象
proxyauth_handler = urllib2.ProxyHandler({"http" : "mr_mao_hacker:sffqry9r@61.158.163.130:16816"})

# 2. 通过 build_opener()方法使用这个代理Handler对象，创建自定义opener对象，参数包括构建的 proxy_handler
opener = urllib2.build_opener(proxyauth_handler)

# 3. 构造Request 请求
request = urllib2.Request("http://www.baidu.com/")

# 4. 使用自定义opener发送请求
response = opener.open(request)

# 5. 打印响应内容
print response.read()
```

### HTTPBasicAuthHandler

有些Web服务器（包括HTTP/FTP等）的有些页面并不想提供公共访问权限，或者某些页面不希望公开，但是可以让特定的客户端访问。那么用户在访问时会要求进行身份认证

```
import urllib
import urllib2

# 用户名
user = "test"
# 密码
passwd = "123456"
# Web服务器 IP
webserver = "http://192.168.199.107"

# 1. 构建一个密码管理对象，用来保存需要处理的用户名和密码
passwdmgr = urllib2.HTTPPasswordMgrWithDefaultRealm()

# 2. 添加账户信息，第一个参数realm是与远程服务器相关的域信息，一般没人管它都是写None，后面三个参数分别是 Web服务器、用户名、密码
passwdmgr.add_password(None, webserver, user, passwd)

# 3. 构建一个HTTP基础用户名/密码验证的HTTPBasicAuthHandler处理器对象，参数是创建的密码管理对象
httpauth_handler = urllib2.HTTPBasicAuthHandler(passwdmgr)

# 4. 通过 build_opener()方法使用这些代理Handler对象，创建自定义opener对象，参数包括构建的 proxy_handler
opener = urllib2.build_opener(httpauth_handler)

# 5. 可以选择通过install_opener()方法定义opener为全局opener
urllib2.install_opener(opener)

# 6. 构建 Request对象
request = urllib2.Request("http://192.168.199.107")

# 7. 定义opener为全局opener后，可直接使用urlopen()发送请求
response = urllib2.urlopen(request)

# 8. 打印响应内容
print response.read()
```

### Cookie

```
# 获取一个有登录信息的Cookie模拟登陆

import urllib2

# 1.url
    url = 'http://www.renren.com/410043129/profile'
    # 添加cookie的原因 是服务器根据cookie判断请求是否合法
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko",
               "Cookie": 'anonymid=je4y320we6q6o9; first_login_flag=1; ln_uact=mr_mao_hacker@163.com; ln_hurl=http://hdn.xnimg.cn/photos/hdn421/20171230/1635/main_JQzq_ae7b0000a8791986.jpg; loginfrom=syshome; ch_id=10016; wp_fold=0; jebe_key=5118cd04-43da-43e9-ab57-591891f461c0%7Cc13c37f53bca9e1e7132d4b58ce00fa3%7C1519693191782%7C1%7C1519693191459; springskin=set; vip=1; depovince=GW; jebecookies=a1223893-d930-40d8-affe-a78eb3cc0d96|||||; _r01_=1; JSESSIONID=abcbF5bIdV9l_fgZuqvhw; ick_login=ec80877d-982a-4a6b-9438-b0e4d58bc971; _de=BF09EE3A28DED52E6B65F6A4705D973F1383380866D39FF5; p=50030f6dd8c40240b2ee83e04db6007d9; t=538dac62dc7092b5a4405e5a86a1f1f09; societyguester=538dac62dc7092b5a4405e5a86a1f1f09; id=327550029; xnsid=2abe6093'
               }

# 2. 通过headers里的报头信息（主要是Cookie信息），构建Request对象
urllib2.Request(url, headers = headers)

# 3. 直接访问renren主页，服务器会根据headers报头信息（主要是Cookie信息），判断这是一个已经登录的用户，并返回相应的页面
response = urllib2.urlopen(request)

# 4. 打印响应内容
print response.read()
```

### HTTPCookieProcessor

```
# coding:utf-8
import urllib2
import cookielib
import urllib


def load_profile():
    # 添加cookie的原因是服务器根据cookie判断请求是否合法
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko",}

    # 创建cookjar对象，用来自动保存cookie
    cookjar = cookielib.CookieJar()

    # 2.创建带有cookie功能的处理器
    cookie_hndler = urllib2.HTTPCookieProcessor(cookjar)

    # 2.创建openner
    openner = urllib2.build_opener(cookie_hndler)

    # 1.代码登录 模拟登录，目的是：获取登录成功之后的cookie
    login_url = 'http://www.renren.com/PLogin.do'
    formdata = {
        "email": "mr_mao_hacker@163.com",
        "password": "alarmchime"
    }
    # 转译 url
    formdata_str = urllib.urlencode(formdata)
    # 构建Request请求对象，包含需要发送的用户名和密码
    login_request = urllib2.Request(login_url, data=formdata_str, headers=headers)

    # 只要登录成功 openner 自动保存了cookie
    openner.open(login_request)

    # 2.带着 cookie 请求好友页面
    # 1.url
    url = 'http://www.renren.com/410043129/profile'
    # 2.Request
    request = urllib2.Request(url, headers=headers)
    # 3.发送请求
    response = openner.open(request)
    # 4. 写入本地
    with open('raw_info/07add.html', 'w') as f:
        f.write(response.read())


if __name__ == '__main__':
    load_profile()
```

## Error

由于HTTPError的父类是URLError，所以父类的异常应当写到子类异常的后面

```
mport urllib2

request = urllib2.Request("http://www.itcast.cn/blog") 

try:
    urllib2.urlopen(request)

except urllib2.HTTPError, err:
    print err.code

except urllib2.URLError, err:
    print err

else:
    print "Good Job"
```

