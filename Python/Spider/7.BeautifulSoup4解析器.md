# BeautifulSoup4

和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。

```
lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。

BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。

Beautiful Soup 3 目前已经停止开发，推荐现在的项目使用Beautiful Soup 4。
```

**官方文档 **

http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0

**安装 **

```
pip install beautifulsoup4
```

**抓取工具**

| 抓取工具      | 速度 | 使用难度 | 安装难度   |
| ------------- | ---- | -------- | ---------- |
| 正则          | 最快 | 困难     | 无（内置） |
| BeautifulSoup | 慢   | 最简单   | 简单       |
| lxml          | 快   | 简单     | 一般       |

## 实例

```
from bs4 import BeautifulSoup

html = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""

#创建 Beautiful Soup 对象
soup = BeautifulSoup(html, "lxml")

#打开本地 HTML 文件的方式来创建对象
#soup = BeautifulSoup(open('index.html'))

#格式化输出 soup 对象的内容
print soup.prettify()
```

## 四大对象

Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:

- Tag(标签)
- BeautifulSoup(特殊Tag)
- NavigableString(标签内容)
- Comment(注释内容)

## 遍历文档树

- .contents(子节点内容)
- .children(子节点)
- .descendants(子孙节点)
- .string(节点内容)

```
# coding:utf-8
from bs4 import BeautifulSoup


def main():
    data = '''
           <html><head><title>The Dormouse's story</title></head>
           <body>
           <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
           <p class="story">Once upon a time there were three little sisters; and their names were
           <a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
           <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
           <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
           and they lived at the bottom of a well.</p>
           <p class="story">...</p>
       '''
    # 1.转换类型
    soup = BeautifulSoup(data, 'lxml')

    # Tag 标签对象,类型是bs4.element.Tag
    result = soup.title
    result = soup.a
    # Tag标签有属性：name、attrs
    # name即为[document]
    result = soup.name
    # head #对于其他内部标签，输出的值便为标签本身的名称
    result = soup.head.name
    # p 标签的所有属性打印输出 --->dict
    result = soup.p.attrs
    # ['title'] #还可以利用get方法，传入属性的名称，二者是等价的
    soup.p['class'] 
    # soup.p.get('class')
    # 可以对这些属性和内容等等进行修改
    soup.p['class'] = "newClass"
    # 还可以对这个属性进行删除
    del soup.p['class'] 

    # navigablestring 标签包含的文本
    result = soup.title.string

    # BeautifulSoup表示文档内容，可以当成特殊Tag
    result = soup
    result = type(soup.name)
    result = soup.name 
    result = soup.attrs  # {}

    # Comment 标签包裹的注释内容，输出不包裹注释符号,特殊navigablestring
    resutl = soup.a.string
    result = type(soup.a.string)
    
    # string 节点内容
    # 一个标签里面没有标签或只有唯一的一个标签，那么 .string 就会返回标签里面的内容。
    resutl = soup.head.string

    # contents 将tag的子节点输出 -->list
    result = soup.body.contents

    # children tag的子节点输出 -->迭代对象
    result = soup.body.children
    for name in result:
        print name
    
    # descendants 所有tag的子孙节点
    for child in soup.descendants:
    	print child

    print type(result)
    print result


if __name__ == '__main__':
    main()
```

## 搜索文档树

- find
- find_all
- select

```
# coding:utf-8
from bs4 import BeautifulSoup
import re


def main():
    data = '''
           <html><head><title>The Dormouse's story</title></head>
           <body>
           <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
           <p class="story">Once upon a time there were three little sisters; and their names were
           <a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
           <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
           <a href="http://example.com/tillie" clas="sister" id="link3">Tillie</a>;
           and they lived at the bottom of a well.</p>
           <p class="story">...</p>
       '''

    # 类型
    soup = BeautifulSoup(data, 'lxml')

    # find 只查找符合的第一个
    # 标签名字
    result = soup.find("a")
    # 属性
    result = soup.find(attrs={"id": "link2"})
    # text
    result = soup.find(text="...")
    # 正则
    result = soup.find(re.compile(r'^a'))

    # findall 查找所有--->list
    result = soup.find_all("a")[1]
    result = soup.find_all(["a", "b"])
    result = soup.find_all(re.compile("^b"))
    result = soup.find_all(class_ = "sister")
    result = soup.find_all(text=["Tillie", "Elsie", "Lacie"])
    result = soup.find_all("p", attrs={"class": "title"})

    # select css的选择器 查找所有 -->list
    # css选择器:标签，类， ID, 组选择器，层级选择器， 属性选择器
    result = soup.select('a')  # 标签
    result = soup.select('.title')  # 类名
    result = soup.select('#link3')	# id
    result = soup.select('a, head')	# 组选择器
    result = soup.select('p a')  # 层级选择器
    result = soup.select('p[name="dromouse"]')  # 属性选择器


    # 获取标签包裹的内容
    # result = result[0].get_text()

    # 获取属性 --->只有class返回的是list，其他属性时字符串
    result = result[0].get('class')
    # result = result[0].get('name')

    print result



if __name__ == '__main__':
    main()
```

## 案例

以腾讯社招页面来做演示：<http://hr.tencent.com/position.php?&start=10#a>

使用BeautifuSoup4解析器，将招聘网页上的职位名称、职位类别、招聘人数、工作地点、发布时间，以及每个职位详情的点击链接存储出来。

```
from bs4 import BeautifulSoup
import urllib2
import urllib
import json    # 使用了json格式存储

def tencent():
    url = 'http://hr.tencent.com/'
    request = urllib2.Request(url + 'position.php?&start=10#a')
    response =urllib2.urlopen(request)
    resHtml = response.read()

    output =open('tencent.json','w')

    html = BeautifulSoup(resHtml,'lxml')

# 创建CSS选择器
    result = html.select('tr[class="even"]')
    result2 = html.select('tr[class="odd"]')
    result += result2

    items = []
    for site in result:
        item = {}

        name = site.select('td a')[0].get_text()
        detailLink = site.select('td a')[0].attrs['href']
        catalog = site.select('td')[1].get_text()
        recruitNumber = site.select('td')[2].get_text()
        workLocation = site.select('td')[3].get_text()
        publishTime = site.select('td')[4].get_text()

        item['name'] = name
        item['detailLink'] = url + detailLink
        item['catalog'] = catalog
        item['recruitNumber'] = recruitNumber
        item['publishTime'] = publishTime

        items.append(item)

    # 禁用ascii编码来处理中文
    line = json.dumps(items,ensure_ascii=False)
    # 存储内容时使用utf-8编码
    output.write(line.encode('utf-8'))
    output.close()

if __name__ == "__main__":
   tencent()
```

