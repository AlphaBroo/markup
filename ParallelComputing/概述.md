# 概述

## 并行计算内存架构

根据可同时处理的指令数量与数据量的不同，计算机系统可分为如下四类

这种分类方法也叫做费林分类(Flynn's taxonomy)

```
单指令，单数据	SISD
单指令，多数据	SIMD
多指令，单数据	MISD
多指令，多数据	MIMD
```

- SISD

SISD计算系统是一个单处理器机器，它执行的每个指令都会操作单个数据流，在其系统中，机器指令是顺序处理的

在一个时钟周期中，CPU会执行如下操作：

```
获取：CPU从内存区域获取数据与指令，该内存区域叫做寄存器

解码：CPU对指令进行解码

执行：指令在数据上得到执行，操作结果会被存储到另一个寄存器中
```

当执行阶段完成后，CPU会对自身进行设置，从而开始另一个CPU周期。

在这类计算机中执行的算法是顺序的(串行)，因为他们并不包含任何并行。只拥有单个CPU的硬件系统就是SISD计算机

构成这种架构(冯诺伊曼架构)的主要元素有如下几项

```
中央存储单元	用于存储指令与程序数据
CPU			用于从存储单元中获取指令与数据，它会对指令进行解码并顺序地执行它们
I/O			指程序的输入输出数据
```

传统的单处理器计算机被归类为SISD系统

- MISD

该模型中的n个处理器，每一个都有自己的控制单元，它们共享同一个存储单元。在每个时钟周期内，从内存接收到的数据会被所有处理器同时处理，每个处理器都会按照从其控制单元中所接收到的指令顺序进行处理。在这种情况下，通过对相同的数据执行几个操作实现了并行(指令级并行)。这种架构能有效解决的问题相当特殊的，比如说与数据加密相关的问题等。出于这个原因，计算机MISD并未在商业上流行起来，其更多地是用在智力训练而非实际使用

- SIMD

SIMD计算机包含n个相同的处理器，每个处理器都有自己的本地内存，可以用于存储数据，所有处理器都处于单个指令流的控制之下；此外，还有n个数据流，分别针对每个处理器。在每个步骤中，处理器都会同时工作并执行相同的指令，不过是在不同的数据元素上执行。这是一种数据并行。SIMD架构要比MISD架构更加通用。大量应用所所涉及的诸多问题都可以通过SIMD计算机中的并行算法来解决。另一个有趣的特性是这些计算机所用的算法的设计、分析和实现相对容易。局限在于只有那些可以被分解为一系列子问题的问题(这些子问题要完全一样，每个子问题后面会通过相同的指令集同时解决)才能通过SIMD计算机来解决。对于根据该范式所开发出的超级计算机来说，必须提到的是Connection Machine与MPP。大量SIMD嵌入式单元的现代图形处理器单元(GPU)的出现使得这种计算范式得到更为广泛的使用。

- MIMD

根据费林的分类，这种并行计算机最为通用，也是更为强大的一种。它有n个处理器、n个指令流以及n个数据流。每个处理器都有自己的控制单元与本地内存，从计算角度来说，这使得MIMD架构要比SIMD更为强大。每个处理器都是在它自己的控制单元所发出的指令流的控制下来进行操作的；因此，处理器可以对不同的数据执行不同的程序，可以将一个大问题分解为多个不同的子问题，然后加以解决。MIMD架构师通过线程与进程级别的并行的帮助来实现的。这还意味着，处理器通常会异步执行。这类计算机用于解决那些拥有不规则结构的问题，而SIMD则要求问题的结构要规则才行。时至今日，这种架构已经用于到了很多PC、超级计算机以及计算机网络中。要注意的是，异步算法的设计、分析与实现不是那么容易的事情

 ## 内存组织

在评估并行架构时需要考虑的另一个防霾呢就是内存架构，也称为数据的访问方式。

无论处理单元速度有多快，若内存无法以足够的速度来维护并提供指令和数据，那么在性能上也不会有什么改进。想让内存的响应时间跟上处理器的速度，需要解决存储周期时间，它指的是连续两个操作之间所经过的时间。处理器的周期时间通常要比内存的周期时间短。当处理器开始传输数据时(向内存传输或是从内存获取)，内存将会在整个存储周期内被占用；在这期间，其他设备(I/O控制器、处理器、甚至是发出该请求的处理器自身)都无法使用内存，因为它要对请求作出响应。

对内存访问问题的解决方案有两个：共享内存系统和分布式内存系统。在共享内存系统中，有一个高层的虚拟内存，所有处理器都可以访问该内存中的数据与指令。在分布式内存系统中，则使用了分布式内存模型，其中每个处理器都有自己的本地内存，其他处理器是无法访问的。共享内存和分布式内存之间的差别在于虚拟内存或是从处理器的视角所看到的内存结构。从物理上来说，几乎每个系统内存都会被划分为不同的组建，它们之间的访问是独立的。共享内存与分布式内存的区别在与处理器单元的内存访问管理方式。

- 共享内存

共享内存的物理连接是相当简单的，总线结构可以支持共享相同通道的任意数量的设备。总线协议最初被设计为支持单个处理器，一个或多个磁盘一斤磁带处理器可以通过这里的共享内存进行通信。注意，每个处理器都会有一个与之相关联的高速缓存，因为很多时候处理器都需要本地内存中的数据或是指令。当一个处理器修改了同时被其他处理器所用的内存系统中的数据时就会产生问题。新值需要从处理器缓存中传递过来，而处理器缓存中的值已经被修改到了共享内存中；不过，接下来它需要传递给所有其他的处理器，这样老的值就无法正常使用了。这个问题叫做缓存一致性问题，这是内存一致性的一个特例，需要硬件实现来处理并发问题与同步，类似于线程编程一样。

共享内存系统的主要特点如下

```
- 内存对于所有处理器来说都是一样的，比如，与相同数据结构相关联的所有处理器都会使用同样的逻辑内存地址，这样就会访问到相同的内存位置

- 同步是通过控制处理器对共享内存的访问实现的。实际上，在某一时刻只有一个处理器能够访问到内存资源

- 当一个任务在访问共享内存时，另一个任务是不可以修改共享内存的位置的

- 数据共享是非常快的，两个任务间通信所需的时间等于读取单个内存位置的时间(取决于内存访问的速度)
```

共享内存系统中的内存访问如下

```
- 统一内存访问(UMA)
该系统的基本特点是，对于每个处理器以及内存的任何区域来说，对内存的访问时间都是恒定的。出于这个原因，这些系统又叫做对称多处理器(SMP)。这些系统相对来说比较容易实现，不过可伸缩型并不好，程序员需要负责同步的管理，这是通过在管理资源的程序中插入恰当的控制、信号量以及锁来实现

- 非统一内存访问(NUMA)
该架构将内存区域划分为高速访问区域(分配给每个处理器，是数据交换的公共区域)以及低速访问区域两种。这些系统又叫做分布式共享内存系统(DSM)。其可伸缩型非常好，不过开发起来难度颇大

- 无远程内存访问(NORMA)
内存在物理上被分配给各个处理器(本地内存)。所有的本地内存都是私有的，只能为本地处理器所访问。处理器之间的通信是通过用于消息交换的通信协议来实现的，叫做消息传递协议

- 仅缓存访问(COMA)
这些系统只有缓存。在分析NUMA架构时，其架构将数据的副本保存到缓存中，并且这些数据在主内存中还会有一个副本存在。该架构去处了主内存中的副本，只保留缓存，内存在物理上被分配给了各个处理器(本地内存)。所有的本地内存都是私有的，只能为本地处理器访问。处理器之间的通信是通过用于消息交换的通信协议来实现的，叫做消息传递协议
```

- 分布式内存

在分布式内存系统中，内存每个处理器关联在了一起，一个处理器只能访问到它自己的内存。也叫做“多计算机系统”。反映了系统元素本身是小型且完备的处理器于内存系统

这种组织方式优点：

```
1. 在通信总线或是开关层面上不会再出现冲突。每个处理器都可以使用其自己本地内存的全部宽带而不会妨碍其他处理器。
2. 无公共总线意味着对于处理器的数量不会再有固有的限制，系统的大小只受限于连接的处理器的网络
3. 不存在缓存一致性的问题。每个处理器负责管理自己的数据，不必再考虑副本的更新问题
```

缺点

```
主要的缺点是处理器之间的通信更加难以实现。如果一个处理器需哟啊另一个处理器内存中的数据，那么这两个处理器就需哟啊通过消息传递协议来交换消息。这会引入两个速度降低之源：一个处理器构建消息并向另一个处理器发送消息需要时间；另外，处理器还得停下来管理从其他处理器所接收到的消息。

针对分布式内存机器所设计的程序需要组织为一组独立的任务，这些任务之间通过消息进行通信
```

主要特点

```
- 物理上，内存在处理器之间是分布式的，每个本地内存都只会被其处理器所直接访问
- 同步是通过在处理器将(通信)移动数据(即便只是消息本身亦如此)来实现的
- 本地内存中数据的分割会影响机器的性能---划分的精确性非常重要，因为这样会将CPU之间的通信降到最低。除此之外，用于协调这些分解与组合操作的处理器必须能与对数据结构的每一部分进行操作的处理器高效通信
- 使用消息传递协议，这样CPU就可以通过数据包的交换来彼此通信。消息是信息的离散单元；它们拥有定义明确的身份，这样就可以对其进行区分了
```

> 大规模并行处理

MPP机器由成百上千个处理器组成(在一些机器上的规模可以达到成千上万)，这些处理器之间通过通信网络进行连接。世界上最开的计算机就是基于这样的架构的

> 工作站集群

这些处理系统基于传统计算机，它们之间通过通信网络进行连接。计算集群就属于这类。

在集群架构中，将节点定义为集群中的单个计算单元。对于用户来说，集群是完全透明的。所有硬件与软件的复杂型都被隐藏起来，在访问数据与应用时就好像它们都来自单个节点一样

```
- 容错集群
在该类集群中，节点的活动会被持续监控。当一个节点停止工作时，另一台机器会接管它的活动，其目标旨在通过架构的冗余来确保持续的服务

- 负载均衡集群
在该系统中，工作请求会被发送给活动较少的接待你。这确保了完成整个过程所需的时间会更少一些

- 高性能计算集群
在该系统中，每个接待你都会被配置以提供非常高的性能。整个过程依然会被划分为在多个节点上执行的多个任务，任务时并行化的，并且分布在不同的机器上。
```

> 异构架构

超级计算机中同构世界中引入的GPU加速器改变了之前超级计算机使用与编程方式的本质。尽管GPU提供了很高的性能，不过它们无法作为一种自洽的处理单元，因为它们总是要与CPU协同使用才行。因此，编程范式非常简单：CPU以串行方式进行控制与计算，将计算代价高昂并且需要很高并行度的任务分配给图形加速器。CPU与GPU之间的通信不仅可以通过高速总线来实现，还可以通过针对物理或是虚拟的单个内存区域的共享来实现。实际上，如果两台设备都没有自己的内存区域，那么可以通过各种编程模型所提供的软件库来访问共享内存区域，如CUDA和OpenCL。这种架构叫做异构架构。其中应用可以在单个地址空间中创建数据结构，并将任务发送给适合于其解析的设备硬件。多个处理任务可以在相同区域中安全地操作以避免数据一致性问题，这要归功于原子操作。因此，虽然CPU和GPU之间的协同效率不高，但借助这种新型架构，可以优化它们之间的交互以及并行应用的性能

## 并行编程模型

并行编程模型是硬件与内存架构的一种抽象。实际上，这些模型并非特定的，也没有指代特定类型的机器或是内存架构。它们可以在任何类型的机器上实现。相比于之前的划分来说，这些编程模型位于更高的层级上，表示软件必须被实现为执行一种并行计算的方式。每种模型都有与其他处理器共享信息的方式，从而访问内存并划分任务

- 共享内存模型

在该模型中，任务共享单个共享的内存区域，对共享资源的访问(读写数据)是异步的。有机制可以让程序员控制对共享内存的访问，比如锁或信号量。该模型的优势在于程序员不必清楚任务间的通信。从性能角度来说，该模型的一个严重缺点是使理解与管理数据的局部性变得更为困难；让数据成为使用它的处理器的局部数据可以减少内存访问、缓存刷新，以及多个处理器使用相同数据时所产生的总线流量

- 多线程模型

在该模型中，一个进程可以拥有多个执行流，比如，先常见一个顺序部分，随后创建一个系列任务并行执行。通常，这种模型会用在共享内存架构中。因此，管理线程间的同步非常重要，因为这些线程会操作共享内存，程序员必须防止多个线程同时更新相同的位置。现代的CPU在软件和硬件上都是多线程的。Posix线程就是软件多线程实现的经典例子。英特尔的超线程技术实现了硬件的多线程，如果一个线程停下来或是等待I/O操作，那么它会切换至另外一个线程。即便数据对齐是非线形的，我们也可以通过该模型实现并行

- 消息传递模型

消息传递模型通常用子啊每个处理器都有自己的内存(分布式内存系统)的场景下。更多的任务可以驻留在同一台物理机或是任意数量的机器上。程序员负责决定并行以及通过消息所进行的数据交换。该并行模型的实现需要在代码中用到特殊的软件库。目前已经有了大量的消息传递模型的实现。MPI(消息传递接口)模型的设计使用了分布式内存，但却成为并行编程的模型，多个平台也可以使用共享内存机器。

- 数据并行模型

在该模型中，多个任务可以操作相同的数据结构，不过每个任务都只会操作数据的不同部分。在共享内存架构中，所有的任务都可以通过共享内存与分布式内存架构来访问数据，其中的数据结构会被分割并驻留在每个任务的本次内存中。为了实现该模型，程序员需要开发一个程序来指定数据的分布与对齐方式。现代GPU在数据对齐的情况下处理速度非常快。

## 如何设计并行程序

- 任务分解

在该阶段，软件程序会被分解为任务或是一组指令，接下来在不同的处理器上执行以实现并行化。为了完成这个分解，可以使用如下方法

```
- 领域分解
将问题数据进行分解；应用对于使用不同部分数据的所有处理器来说是公共的。当待处理的数据量很大时通常会使用该方法

- 功能性分解
在这种情况下，问题被分解为任务，每个任务都会在所有可用数据上执行特定的操作
```

- 任务分配

在这个步骤中，指定好将任务分发到各个处理器上的机制。这个阶段非常重要，因为它会在各个处理器上建立工作负载的分配机制。在这里，负载均衡尤为重要；实际上，所有处理器必须要不间断地工作，从而避免较长时间的闲置状态。为了做到这一点，程序员需要考虑到系统之间可能存在的异构性，从而在性能更好的处理器上分配更多的任务。最后，想获得更改的并行效率，要尽可能地闲置处理器之间的通信，因为这常常是速度变慢以及资源消耗之源

- 聚集

聚集指的是将小任务与大任务合并到一起从而改进性能的过程。如果设计过程的前两个阶段将问题划分为一系列任务，但任务数量远远超过可用的处理器数量，同时计算机并未针对处理大量的小任务而进行特别的设计(有些架构如GPU则可以很好地解决这个问题，实际上还会从运行数以百万甚至数以亿计的任务中获益)，那么设计就是极其低效的。一般来说，这是因为任务需要与处理器或是线程进行通信，这样它们才能计算任务。大多数通信的代价不仅与所传输的数据量有关，每次通信操作也有固定的代价(如建立TCP连接时固有的延迟)。如果任务过小，那么这种固定的代价就很容易使设计变得毫无效率可言

- 映射

在并行算法设计过程的映射阶段，指定哪个任务将要执行。其目标是将总执行时间将到最低。这里必须做出权衡，因为两个主要的策略之间经常会彼此冲突

```
- 通信频繁的任务应该被放到同一个处理器中来增加局部性
- 可以并发执行的任务应该被方法哦不同的处理器中以增强并发性 
```

这就是映射问题，即NP完备。这样，一般来说，并不存在针对问题的多项式时间解决方案。对于相同大小的任务以及很容易确定通信模式的任务来说，映射是很直接的(还可以执行聚集以将映射到相同处理器的任务合并起来)。不过，如果任务的通信模式很难预测或事每个任务的工作量大小千差万别，那么就很难设计出一种高效的映射与聚集模式了，对于这类问题，可以通过负载均衡算法在运行期间确定聚集与映射策略。最难的是那种在程序执行期间通信量或事任务量发生变化的情况。对于这类问题，可以使用动态的负载均衡算法，它会在执行期间周期性地运行

> 动态映射

 不同的问题存在着多种负载均衡算法，有全局的，也有局部的。全局算法需要对待执行的计算有全局的掌控，通常会增加大量的成本。局部算法只以来于特定于任务本身的信息，相对于全局算法来说，这降低了成本，不过在寻找最优的聚集与映射时情况会变得更糟。不过，虽然映射本身效果更差，但降低的成本却会减少执行时间。如果除了执行开始或结束外任务之间很少通信，就可使用任务调度算法来简化处理器空闲时将任务映射到其上的操作。在任务调度算法中会维护一个任务池，任务会被放到这个池中，并由执行者取出

该模型中存在3种常见的方式

```
- 管理者/执行者
这是一种基本的动态映射模式，所有的使用者都会连接到中心化的管理者。管理者会不断向使用者发送任务并收集结果。这种策略对于数量较少的处理器来说可能是最合适的。可以通过提前获取任务使得通信与计算之间彼此重叠来改进该策略

- 层次化的管理者/执行者
这是管理者/执行者的一个变种，它有一个半分布式的层次；执行者被划分到组里，每个都与自己的管理者相关联。这些组管理者会与中央管理者进行通信(组管理者之间也可以通信)，同时执行者会从组管理者那里请求任务。这会将负载分配到几个管理者上，这样如果所有执行者都从同一个管理者请求任务，那么它就可以处理更多数量的处理器了

- 去中心化
在这种模式下，一切都是去中心化的。每个处理器会维护自己的任务池，并与其他处理器通信来请求任务。一个处理器选择其他处理器来请求任务的方式是不同的，这是根据问题来决定的
```

## 如何评估并行程序的性能

并行计算的关注点在于在相对较短的时间内解决大规模问题。影响该目标的因素有所用的硬件类型、问题的并行度以及所用的并行编程模型等。为了实现这一点，引入基本的概念分析，比较从原始序列所获得的并行算法。其性能是通过分析与量化所用的线程数与进程数来达成。

- 加速

加速是一种度量方式，用于展示以并行方式解决问题所带来的好处。它的定义是，在单个处理元素上解决一个问题所花费的时间$T_s$除以在p个相同的处理元素上解决同样问题所花费的时间$T_p​$

用$S= T_s/T_p$来表示加速。如果$S=p$，那么加速就是线性的，这表示如果处理器数量增加，那么执行速度也会增加。当然，这是理想情况。当$T_s$是最佳的串行算法执行时间时，加速时绝对的；当$T_s​$是单个处理器上并行算法的执行时间时，加速是相对的

条件如下

```
S = p	是线性的或理想状况下的加速
S < p	是真实的加速
S > p	是超线性加速
```

- 效率

在理想世界中，拥有p个处理元素的并行系统的加速等于p。通常情况下，一些时间会浪费在空闲或是通信上。效率是一个性能度量，它会估算相比于在通信与同步上所浪费的时间来说，处理器在解决一个任务时的利用率

用$E=S/p=T_s/(pT_p)$表示效率。线性加速算法的E值为1；在其他情况下，E值要小于1。如下列出了E值的3种情况

```
E = 1	线性的
E < 1	真实情况的
E << 1	一个并行效率很低的问题
```

- 可伸缩性

可伸缩性指的是并行机器的效能。是计算能力(并行速度)除以处理器数量的值。如果问题规模变大，同时处理器数量也随之增加，那么性能是不会有损耗的。通过增加不同的因子，可伸缩性系统会保持相同的性能或是改进性能。

- 阿姆达尔定律

阿姆达尔定律是用于设计处理器与并行算法的广泛使用的定律。它阐释了可获取的最大的加速是由程序中的串行组件所决定的$S=\frac{1}{1-P}$，其中，`1-P`表示程序中的串行组件(非并行部分)。这意味着如果一个程序中90%的代码是可并行的，但有10%的代码要保持串行，那么可获取的最大的加速度就是9，即使对于无限数量的处理器来说亦是如此

- 古斯塔夫定律

古斯塔夫定律基于如下假设

```
在增加问题维度时，其串行部分保持不变
在增加处理器数量时，每个处理器所处理的工作保持不变
```

这表明$S(P)=P-\alpha(P-1)$，其中P是处理器数量，S是加速，$\alpha​$是任何并行进程中的非并行部分。它与阿姆达尔定律相反，阿姆达尔定律认为单个进程的执行时间是固定的，并且每个进程的并行执行时间会减少。因此，阿姆达尔定律基于固定问题规模这样一个假设；它假设一个问题的全部工作量并不会随着机器规模(即处理器数量)的变化而变化。古斯塔夫定律着重解决了阿姆达尔定律的缺陷，后者并未将解决任务时所涉及的全部计算资源量考虑在内。它表明，设定并行问题解决方案时间的最佳方式是将所有计算资源都考虑进来，基于这一点，它解决了阿姆达尔定律的问题