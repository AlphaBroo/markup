# 特征工程

数据预处理涉及的策略和技术非常广泛，主要技术是属性选择技术和主成分分析技术。

属性选择

```
指从数据集中选择最具代表性的属性子集，删除冗余或不相关的属性，从而提高数据处理的效率，使模型更容易理解
```

主成分分析

```
利用降维的思想，把给定的一组相关属性通过线性变换转换成另一组不相关的属性，这些新属性按照方差依次递减的顺序进行排列
主成分分析将很多个复杂属性归结为少数几个主成分，将复杂问题简单化，便于分析和处理
```

离散化

```
将连续的数值型数据切分为若干个称为分箱(bin)的小段，是数据分析中常用的手段
离散化的实质是将无限空间中有限的个体映射到有限的空间中去，其作用是提高算法的时空效率，有时也为了能够使用特定的机器学习算法而将数据离散化
```

## 数据                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

离散型数据：由于记录不同类别个体的数目所获得的数据，又称计数数据，不能再细分，也不能提高精度

连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，这类数据通常是非整数，含有小数部分

- 可用数据集

Kaggle网址：<https://www.kaggle.com/datasets>

UCI数据集网址：<http://archive.ics.uci.edu/ml/>

scikit-learn网址：[http://scikit-learn.org/stable/datasets/index.html#datasets](http://scikit-learn.org/stable/datasets/index.html)

## scikit-learn

- 安装

```python
# 创建一个基于python3的虚环境
mkvirtualenv -p /usr/bin/python3.6 目录
# 在ubuntu的虚环境中运行
pip install Numpy
pip install scipy
pip install matplotlib
pip install Scikit-learn
# 通过命令查看
import sklearn
```

- 数据集导入

```python
# 加载并返回鸢尾花数据集
sklearn.datasets.load_iris()
# 加载并返回波士顿放假数据集
sklearn.datasets.load_boston()
# 加载并返回20类新闻数据集
sklearn.datasets.fetch_20newsgroups(data_home=None,subset='all')
data_home:表示数据集下载的目录,默认是 ~/scikit_learn_data/
subset: 'all'，'train'或'test'，可选，选择要加载的数据集.
      训练集的“训练”，测试集的“测试”，两者的“全部”
datasets.clear_data_home(data_home=None)
	清除目录下的数据

# 属性
DESCR			---> 数据集描述
feature_names	---> 特征名
data			---> 特征值数据数组，是[n_samples*n_features]的二维numpy.ndarry数组
target_names	---> 标签名，回归数据集没有
target			---> 目标值数组
```

## 特征抽取

- 英文文本

```python
# 对文本数据进行特征值化
# 类
sklearn.feature_extraction.text.CountVectorizer

# 实例化
CountVectorizer()

CountVectorizer.fit_transform(X)       
X:文本或者包含文本字符串的可迭代对象
返回词频矩阵

CountVectorizer.get_feature_names()
返回值:单词列表
```

实现

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

# 特征抽取，抽取词频矩阵
cv = CountVectorizer()
# fit提取特征名
name = cv.fit(data)
# transform根据提取出来的特征词，统计个数
result = cv.transform(data)
# fit_transform = fit + transform
# data是文本或包含文本字符串的可迭代对象，返回词频矩阵
# result = cv.fit_transform(data)
# 返回单词列表
print(cv.get_feature_names())
# 稀疏矩阵
print(result)
# sparse矩阵转换为array数组
print(result.toarray())
```

- 字典抽取

```python
# 对字典数据进行特征值化

# 类
sklearn.feature_extraction.DictVectorizer
# 实例化
DictVectorizer(sparse=True,…)

DictVectorizer.fit_transform(X)       
X:字典或者包含字典的迭代器
返回值：返回sparse矩阵

DictVectorizer.inverse_transform(X)
X:array数组或者sparse矩阵
返回值:转换之前数据格式

DictVectorizer.get_feature_names()
返回类别名称

DictVectorizer.transform(X)
按照原先的标准转换
```

实现

```python
from sklearn.feature_extraction import DictVectorizer

data = [{'city': '北京', 'temperature': 100},
        {'city': '上海', 'temperature': 60},
        {'city': '深圳', 'temperature': 30}]
# 字典数据特征抽取，默认稀疏矩阵
dv = DictVectorizer(sparse=False)
# 提取特征名及词频
# 输入是字典或者包含字典的迭代器，返回值是sparse矩阵
result = dv.fit_transform(data)
# dv.fit(data)
# result = dv.transform(data)
# 输入是array数组后者sparse矩阵，返回值是转换之前数据格式
origin = dv.inverse_transform(result)
# 返回了表的名称
print(dv.get_feature_names())
print(result)
print(origin)
```

- one-hot

```
3种方式：
1. DictVectorizer(注意，数字不会进行转换)
2. OneHotEncoder(Numpy)
3. pd.get_dummies(Pandas)
```

实现

```python
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import OneHotEncoder


df = pd.read_csv("./one_hot_test.csv")
# print(df)

# 1.DictVecotrizer
# df.gender = df.gender.map({0: "female", 1: "male", })
# print(type(df))
# print(df["gender"])  # Series对象
# print(df[["gender"]]) # DataFrame对象
# print(list(df[["gender"]].T.to_dict().values()))
# data = list(df[["gender"]].T.to_dict().values())
# dv = DictVectorizer(sparse=False)
# result = dv.fit_transform(data)
# print(dv.get_feature_names())
# print(result)

# 2.OneHotEncoder
encoder = OneHotEncoder(sparse=False)
result = encoder.fit_transform(df[["gender"]])
print(result)

# 3.pd.get_dummies转换之后不需要在做合并
data = pd.get_dummies(data=df, columns=["gender"])
print(data)
```

- 中文文本

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

data = "生活很短，我喜欢python, 生活太久了，我不喜欢python"

# 分词,返回值是generator
cut_ge = jieba.cut(data)
# 方法一：生成器转列表
# content = []
# for word in cut_ge:
#     content.append(word)
# data = [" ".join(content)]
# 方法二，join(可迭代)
data = " ".join(cut_ge)
cv = CountVectorizer()
result = cv.fit_transform(data)
print(cv.get_feature_names())
print(result)
print(result.toarray())
```

- TF-IDF

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

```python
# 类
sklearn.feature_extraction.text.TfidfVectorizer
# 返回词的权重矩阵
TfidfVectorizer(stop_words=None,…)

TfidfVectorizer.fit_transform(X,y)       
X:文本或者包含文本字符串的可迭代对象
返回值：返回sparse矩阵

TfidfVectorizer.inverse_transform(X)
X:array数组或者sparse矩阵
返回值:转换之前数据格式

TfidfVectorizer.get_feature_names()
返回值:单词列表
```

实现

```python
import jieba 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cut_words():
    s1 = "今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    s2 = "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"
    s3 = "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"   
    s1_ge = jieba.cut(s1)
    s2_ge = jieba.cut(s2)
    s3_ge = jieba.cut(s3)
    return " ".join(s1_ge), " ".join(s2_ge), " ".join(s3_ge)

words1, words2, words3 = cut_words()
# 使用TFIDF特征抽取
tfidf = TfidfVectorizer(stop_words=["一种", "每个"])
# 输入：文本或包含文本字符创的可迭代对象，返回值：saprse矩阵
result = tfidf.fit_transform([words1, words2, words3])
# 返回值：单词列表
print(tfidf.get_feature_names())
print(result.toarray())
# 输入：array数组或sparse矩阵，返回值：转换之前的数据格式
print(tfidf.inverse_transform(result))
```

## 特征处理

### 归一化

```python
# 类
sklearn.preprocessing.MinMaxScaler
# 实例化
MinMaxScaler(feature_range=(0,1)…)
每个特征缩放到给定范围(默认[0,1])

MinMaxScaler.fit_transform(X)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后的形状相同的array
```

实现

```python
from sklearn.preprocessing import MinMaxScaler

data = [[90, 2, 10, 40],
        [60, 4, 15, 45],
        [75, 3, 13, 46]]

# 将每个特征值缩放到给定范围，默认[0,1]
mm = MinMaxScaler()
# 输入ndarray，返回转换后的形状相同的array
result = mm.fit_transform(data)
# print(mm.fit(data))
# print(mm.transform(data))
print(mm.data_min_)
print(mm.data_max_)
```

### 标准值化

```python
# 类
scikit-learn.preprocessing.StandardScaler
# 实例化
StandardScaler(…)
处理之后每列来说所有数据都聚集在均值0附近方差为1

StandardScaler.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后的形状相同的array

StandardScaler.mean_
原始数据中每列特征的平均值
```

实现

```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

iris = load_iris()
# 处理之后每列的数据都聚集在均值0，方差1附近
ss = StandardScaler()
# 输入：ndarray，输出:转换后形状相同的array
result = ss.fit_transform(iris.data)
print(result)
```

### 缺失值

```python
# 类
sklearn.preprocessing.Imputer
# 实例化
Imputer(missing_values='NaN', strategy='mean', axis=0)
完成缺失值插补

Imputer.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后的形状相同的array
```

实现

```python
import numpy as np
from sklearn.preprocessing import Imputer

data = [[1, 2],
        [np.nan, 3],
        [7, 6]]

# 完成缺失值插补
imputer = Imputer(missing_values="NaN", strategy="mean", axis=1)
# 输入numpy array格式的数据，返回转换后形状相同的array
result = imputer.fit_transform(data)
print(result)
print(result.shape
```

### 特征选择

主要方法：

```
Filter:VarianceThreshold

Embedded:正则化、决策树

Wrapper

```

函数

```
# 类
sklearn.feature_selection.VarianceThreshold
# 实例化
VarianceThreshold(threshold = 0.0)
删除所有低方差特征

Variance.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：训练集差异低于threshold的特征将被删除。
默认值是保留所有非零方差特征，即删除所有样本
中具有相同值的特征。
```

实现

```python
from sklearn.feature_selection import VarianceThreshold

data = [[0, 2, 0, 3],
        [0, 1, 4, 3],
        [0, 1, 1, 3]]
        
# 删除所有低方差特征，默认0.0
vt = VarianceThreshold()
# 输入值：numpy array格式数据
# 返回值：训练集差异低于threshold的特征将被删除
result = vt.fit_transform(data)
print(result)
print(result.shape)
```

### 降维

分类：

```
主成成分分析(principalcomponent analysis,PCA)

因子分析(Factor Analysis)

独立成分分析(Independent Component Analysis，ICA)
```

本质：PCA是一种分析、简化数据集的技术。在PCA中，数据从原来的坐标系转换到新的坐标系。

目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。

函数

```python
# 类
sklearn.decomposition.PCA
# 实例化
PCA(n_components=None)
将数据分解为较低维数空间

PCA.fit_transform(X)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后指定维度的array
```

实现

```python
from sklearn.decomposition import PCA

data = [[2, 8, 4, 5],
        [6, 3, 0, 8],
        [5, 4, 9, 1]]

# 将数据分解为较低维数空间
pca = PCA(n_components=3)
# 输入：numpy array格式的数据，返回值：转换后制定维度的array
result = pca.fit_transform(data)
# pca降维之后，新的数据具体意义就丧失了，主要信息保留
print(result)
```

**特征选择/降维**

相同点：

特征选择和降维都是降低数据维度

不同点：

特征选择筛选掉的特征不会对模型的训练产生任何影响

降维做了数据的映射，保留主要成分，所有的特征对模型训练有影响

## 使用流程

流程

```
1.导入需要的库
2.导入数据集
3.处理丢失数据
4.解析分类数据
5.拆分数据集为训练集合和测试集合
6.特征量化
```



data

| Country | Age  | Salary | Purchased |
| ------- | ---- | ------ | --------- |
| France  | 44   | 72000  | No        |
| Spain   | 27   | 48000  | Yes       |
| Germany | 30   | 54000  | No        |
| Spain   | 38   | 61000  | No        |
| Germany | 40   |        | Yes       |
| France  | 35   | 58000  | Yes       |
| Spain   |      | 52000  | No        |
| France  | 48   | 79000  | Yes       |
| Germany | 50   | 83000  | No        |
| France  | 37   | 67000  | Yes       |

code

```python
#Step 1: Importing the libraries
import numpy as np
import pandas as pd

#Step 2: Importing dataset
dataset = pd.read_csv('../datasets/Data.csv')
X = dataset.iloc[ : , :-1].values
Y = dataset.iloc[ : , 3].values
print("Step 2: Importing dataset")
print("X")
print(X)
print("Y")
print(Y)

#Step 3: Handling the missing data
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
print("---------------------")
print("Step 3: Handling the missing data")
print("step2")
print("X")
print(X)

#Step 4: Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
#Creating a dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
print("---------------------")
print("Step 4: Encoding categorical data")
print("X")
print(X)
print("Y")
print(Y)

#Step 5: Splitting the datasets into training sets and Test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
print("---------------------")
print("Step 5: Splitting the datasets into training sets and Test sets")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
print("Y_train")
print(Y_train)
print("Y_test")
print(Y_test)

#Step 6: Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
print("---------------------")
print("Step 6: Feature Scaling")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
```

