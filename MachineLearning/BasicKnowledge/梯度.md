# 梯度

## 梯度下降

Gradient Descent

不是一个机器学习算法

是一种基于搜索的最优化方法

作用：最小化一个损失函数
$$
-\eta\frac{\mathrm{d}J}{\mathrm{d}\theta}
$$
其中$\eta$

```
为学习率(learning rate)
取值影响获得最优解的速度，甚至收敛性
太小，速度过低
太大，甚至导致不收敛
是梯度下降法的一个超参数
```

注意：并不是所有函数都有唯一的极值点

解决方案

```
1. 多次运行，随机初始化点
2. 梯度下降法的初始点也是一个超参数
```

### 模拟梯度下降

```python
import numpy as np
import matplotlib.pyplot as plt


plot_x = np.linespace(-1, 6, 141)  # theta
plot_y = (plot_x-2.5)**2-1  # 损失函数

def dJ(theta):
  	return 2*(theta - 2.5)
  
def J(theta):
  	try:
  			return (theta-2.5)**2 - 1
    except:
      	return float('inf')  # 极大值

def gradient_descent(initial_theta, eta, n_iters=1e4, epsilon=1e-8):
  	theta = initial_theta
    theta_history.append(initial_theta)
    i_iter = 0
    while i_iter < n_iters:  # 避免无止境
      	gradient = dJ(theta)
    		last_theta = theta
    		theta = theta - eta * gradient
    		theta_history.append(theta)
    
    		if(abs(J(theta) - J(last_theta)) < epsilon)：
    				break
				i_iter += 1
        
    print(theta)
    print(J(theta))
        
def plot_theta_history():
  	plt.plot(plot_x, J(plot_x))
		plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
		plt.show()
    
eta = 0.01  # 可使用不同值来验证
theta_history = []
gradient_descent(0., eta)
plot_theta_history()
```

### 批量梯度下降

Batch Gradient Descent

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ(theta, X_b, y):
  	res = enp.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
      	res[i] = (X_b.dot(theta) - y).doct(X-b[:,i])
    return res * 2 / len(X_b)
  
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta[0], theta[1:])
```

### 随机梯度下降

Stochastic Gradient Descent

优点

```
1. 跳出局部最优解
2. 更快的运行速度
```

实现

```python
import numpy as np
import matplotlib.pyplot as plt

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.
  
def sgd(X_b, y, initial_theta, n_iters):
  	t0 = 5
    t1 = 50
    def learning_rate(t):
      	return t0 / (t + t1)
    theta = initial_theta
    for cur_iter in range(n_iters):
      	rand_i = np.random.randint(len(X_b))
        grdient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * grdient
    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)
print(theta[0], theta[1:])
```

### 小批量梯度下降

Mini-Batch Gradient Descent

批量梯度下降+随机梯度下降

使用K行的样本

## 梯度调试

对于曲线上某点的导数，可近似认为
$$
\frac{\mathrm{d}J}{\mathrm{d}\theta} = \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}
$$
则
$$
\theta = (\theta_0, \theta_1,\cdots ,\theta_n)
$$

$$
\frac{\partial{J}}{\partial{\theta}} = (\frac{\partial{J}}{\partial{\theta_0}},\frac{\partial{J}}{\partial{\theta_1}},\ldots,\frac{\partial{J}}{\partial{\theta_n}})
$$

有
$$
\theta_1^+ = (\theta_0, \theta_1+\varepsilon,\cdots,\theta_n)
$$

$$
\theta_1^- = (\theta_0, \theta_1+\varepsilon,\cdots,\theta_n)
$$

$$
\frac{\partial{J}}{\partial{\theta_1}} = \frac{J(\theta_1^{+})-J(\theta_1^{-})}{2\varepsilon}
$$



虽然算法复杂度高，但是可用于小批量数据验证梯度下降算法

步骤

```
1. 使用debug模式获得正确解
2. 进行数学求解
3. 比对验证两种结果
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.random(size=(1000, 10))
true_theta = np.arrange(1, 12, dtype=float)
X_b = np.hstack([np.ones((len(X), 1)), X])
y = X_b.dot(true_theta) + np.random.normal(size=1000)

def J(theta, X_b, y):  # 损失函数
  	try:
      	return np.sum((y-X_b.dot(theta)**2)/len(X_b))
    except:
      	return float('inf')
      
def dJ_math(theta, X_b, y):
  	"""数学向量化求导，特定情况"""
		return X_b.dot(X_b.dot(theta) - y) * 2. / len(y)
  
def dJ_debug(theta, X_b, y, epsilon=0.01):
  	"""近似求导，通用"""
  	res = np.empty(len(theta))
    for i in range(len(theta)):
      	theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y))/(2*epsilon)
    return res
  
def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
		"""批量梯度下降法"""
    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
 
X_b = np.hstack([np.ones((len(X), 1)), X])
init_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(dj_debug, X_b, y, initial_theta, eta)
theta = gradient_descent(dj_math, X_b, y, initial_theta, eta)
```

## 梯度上升

最大一个效用函数
$$
+\eta\frac{\mathrm{d}J}{\mathrm{d}\theta}
$$
