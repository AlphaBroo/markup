# 模型相关

## 数据集                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

离散型数据：由于记录不同类别个体的数目所获得的数据，又称计数数据，不能再细分，也不能提高精度

连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，这类数据通常是非整数，含有小数部分

- 可用数据集

Kaggle网址：<https://www.kaggle.com/datasets>

UCI数据集网址：<http://archive.ics.uci.edu/ml/>

scikit-learn网址：[http://scikit-learn.org/stable/datasets/index.html#datasets](http://scikit-learn.org/stable/datasets/index.html)

- 数据集划分

训练数据：用于训练，构建模型

测试数据：用于校验，测试模型是否有效

划分比例：`70:30, 80:20, 75:30`

作用：可以测试模型的泛化能力

```python
# model_selection.py
import numpy as np


def train_test_split(X, y, test_ratio=0.2, seed=None):
    """将数据 X 和 y 按照test_ratio分割成X_train, X_test, y_train, y_test"""
    assert X.shape[0] == y.shape[0], \
        "the size of X must be equal to the size of y"
    assert 0.0 <= test_ratio <= 1.0, \
        "test_ration must be valid"

    if seed:
        np.random.seed(seed)

    shuffled_indexes = np.random.permutation(len(X))

    test_size = int(len(X) * test_ratio)
    test_indexes = shuffled_indexes[:test_size]
    train_indexes = shuffled_indexes[test_size:]

    X_train = X[train_indexes]
    y_train = y[train_indexes]

    X_test = X[test_indexes]
    y_test = y[test_indexes]

    return X_train, X_test, y_train, y_test
```

## 交叉验证

Cross Validation

训练数据：训练模型使用的数据集

验证数据：调整超参数使用的数据集

测试数据：作为衡量最终模型性能的数据集

如果对所有的数据进行模型训练，则可能造成过拟合，上线后无法进行模型调整，故需要测试数据集

如是仅仅用测试数据集对模型进行验证调整，则可能造成针对特定测试数据集过拟合，故在数据中随机使用部分数据作为验证数据集

有了验证数据，则可以在数据训练时采用验证数据进行验证来调整超参数，之后再用测试数据集进行测试，但是随机带来新的问题，验证数据可能存在极端情况，会造成对验证数据集的过拟合，故需要交叉验证

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cros_val_score

digits = datasets.load_digits()
X = digits.data
y = digits.target

# 测试数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)

# 手动循环超参数验证
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

# 交叉验证
knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.mean(scores)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2 p=2)
best_knn_clf.fit(X_train, y_train)
score = best_knn_clf.score(X_test, y_test)
print(score)
```







## 评价

### KNN

准确度
$$
accuracy = \frac{\sum_{i=1}^m{(y_{test}^{(i)}==\hat{y}_{test}^{(i)})}}{m}
$$

### 线性回归

均方误差(Mean Squared Error)
$$
MSE = \frac{\sum_{i=1}^m{(y_{test}^{(i)}-\hat{y}_{test}^{(i)})^2}}{m}
$$


根均方误差(Root Mean Squared Error)

```
可降低量纲的影响
```

$$
RMSE = \sqrt{MSE}
$$

平均绝对误差(Mean Absolute Error)
$$
MAE = \frac{\arrowvert y_{test}^{(i)}-\hat{y}_{test}^{(i)}\arrowvert}{m}
$$
R方

```
可使用于不同量纲(业务)情况下比较,小于等于1，
越大表示模型越好，
等于基准模型时为0，
小于0时表示训练的模型还不如基准模型，可能不存在任何线性关系
```

$$
R^2 = 1 - \frac{SS_{redidual}}{SS_{total}}=1 - \frac{\sum_i{(\hat{y}^{(i)}-y^{(i)})^2}}{\sum_i{(\bar{y}^{(i)}-y^{(i)})^2}} =1 - \frac{(\sum_{i=1}^m{(\hat{y}^{(i)}-y^{(i)})^2})/m}{(\sum_{i=1}^m{(\bar{y}^{(i)}-y^{(i)})^2})/m}= 1 - \frac{MSE(\hat{y}, y)}{Var(y)}
$$



示例

```python
# metrics.py
import numpy as np
from math import sqrt


def accuracy_score(y_true, y_predict):
    """计算y_true和y_predict之间的准确率"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(y_true == y_predict) / len(y_true)


def mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的MSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum((y_true - y_predict)**2) / len(y_true)


def root_mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""

    return sqrt(mean_squared_error(y_true, y_predict))


def mean_absolute_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)


def r2_score(y_true, y_predict):
    """计算y_true和y_predict之间的R Square"""

    return 1 - mean_squared_error(y_true, y_predict)/np.var(y_true)

```





## 超参数

模型参数：算法过程中学习的参数

超参数：在算法运行前需要决定的参数

超参数选择

```
1. 领域知识
2. 经验数值
3. 实验搜索
```

实验搜索

```python
best_method = ""
best_score = 0.0
best_k = -1
for method in ["uniform", "distance"]:
		for k in range(1, 11):
  			knn_clf = KNeighborsClassifier(n_neighbors=k)
    		knn_clf.fit(X_train, y_train)
    		score = knn_clf.score(X_test, y_test)
    		if score > best_score:
      			best_k = k
      			best_score = score
            best_method = method
print("best_k = ", best_k)
print("best_score = ", best_score)
print("best_method = ", best_method)
```

## 拟合泛化

过拟合

```
算法所训练的模型过多地表达了数据间的噪音关系
```

欠拟合

```
算法所训练的模型不能完整表述数据间的关系
```

数据集分为测试数据集和训练数据集，可以提高模型的泛化能力

## 学习曲线

随着训练样本的逐渐增多，算法训练出的模型的表现能力

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler


np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * X ** 2 + x * 2 + np.random.normal(0, 1, size = 100)

plt.scatter(x, y)
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
print(X_train.shape)

train_score = []
test_score = []
for in in range(1, 76):
  	lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    y_train_predict = lin_reg.predict(X_tain[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
    
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

# 封装函数
def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
		train_score = []
		test_score = []
		for in in range(1, 76):
    		algo.fit(X_train[:i], y_train[:i])
    		y_train_predict = algo.predict(X_tain[:i])
    		train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    		y_test_predict = algo.predict(X_test)
    		test_score.append(mean_squared_error(y_test, y_test_predict))
    
		plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
		plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
		plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
		plt.show()
# 线性回归调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)
# 多项式回归调用
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)
# 过拟合调用
poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)
```





