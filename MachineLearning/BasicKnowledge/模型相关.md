# 模型相关

## 数据集                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

离散型数据：由于记录不同类别个体的数目所获得的数据，又称计数数据，不能再细分，也不能提高精度

连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，这类数据通常是非整数，含有小数部分

- 可用数据集

> 流行的开发数据存储库

Kaggle网址：<https://www.kaggle.com/datasets>

UCI数据集网址：<http://archive.ics.uci.edu/ml/>

Amazon的AWS数据集：<http://aws.amazon.com/fr/datasets/>

scikit-learn网址：[http://scikit-learn.org/stable/datasets/index.html#datasets](http://scikit-learn.org/stable/datasets/index.html)

> 元门户网站

<http://dataportals.org/>

<http://opendatamonitor.eu/>

<http://quandl.com/>

> 其他页面

维基百科的机器学习数据集<https://goo.gl/SJHN2k>

Quora.com 问题<http:/goo.gl/zDR78y>

Datasets subreddit<https://www.reddit.com/datasets>

- 数据集划分

训练数据：用于训练，构建模型

测试数据：用于校验，测试模型是否有效

划分比例：`70:30, 80:20, 75:30`

作用：可以测试模型的泛化能力

```python
# model_selection.py
import numpy as np


def train_test_split(X, y, test_ratio=0.2, seed=None):
    """将数据 X 和 y 按照test_ratio分割成X_train, X_test, y_train, y_test"""
    assert X.shape[0] == y.shape[0], \
        "the size of X must be equal to the size of y"
    assert 0.0 <= test_ratio <= 1.0, \
        "test_ration must be valid"

    if seed:
        np.random.seed(seed)

    shuffled_indexes = np.random.permutation(len(X))

    test_size = int(len(X) * test_ratio)
    test_indexes = shuffled_indexes[:test_size]
    train_indexes = shuffled_indexes[test_size:]

    X_train = X[train_indexes]
    y_train = y[train_indexes]

    X_test = X[test_indexes]
    y_test = y[test_indexes]

    return X_train, X_test, y_train, y_test
```

## 交叉验证

Cross Validation

训练数据：训练模型使用的数据集

验证数据：调整超参数使用的数据集

测试数据：作为衡量最终模型性能的数据集

如果对所有的数据进行模型训练，则可能造成过拟合，上线后无法进行模型调整，故需要测试数据集

如是仅仅用测试数据集对模型进行验证调整，则可能造成针对特定测试数据集过拟合，故在数据中随机使用部分数据作为验证数据集

有了验证数据，则可以在数据训练时采用验证数据进行验证来调整超参数，之后再用测试数据集进行测试，但是随机带来新的问题，验证数据可能存在极端情况，会造成对验证数据集的过拟合，故需要交叉验证

缺点

```
把训练集分成k份，称为k-folds cross validation，每次训练K个模型，整体性能慢了k倍
```

留一法

```
LOO-CV
有m个样本，把训练数据集分成m份， 称为留一法，Leave-One-Out Cross Validation

优点：
完全不受随机的影响，最接近模型真正的性能指标
缺点：
计算量大
```

示例

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

digits = datasets.load_digits()
X = digits.data
y = digits.target

# 测试数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)

# 手动循环超参数验证
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

# 交叉验证
knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
# cross_val_score(knn_clf, X_train, y_train, cv=5)  # 指定训练集分割份数
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.mean(scores)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2 p=2)
best_knn_clf.fit(X_train, y_train)
score = best_knn_clf.score(X_test, y_test)
print(score)

# 网格搜索
param_grid = [
  {
    'weights': ['distance'],
    'n_neighbors': [ i for i in range(2, 11)],
    'p': [i for i in range(1, 6)]
  }
]
grid_search = GridSearchCV(knn_clf, param_grid, verbose=1)
# GridSearchCV(knn_clf, param_grid, verbose=1, cv=5)  # 指定训练集分割份数
grid_search.fit(X_train, y_train)
print(grid_search.best_score_)
print(grid_search.best_params_)
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
```

## 网格搜索

模型参数：算法过程中学习的参数

超参数：在算法运行前需要决定的参数

超参数选择方法：领域知识、经验数值、实验（网格）搜索

在网格搜索中每组超参数都采用交叉验证来进行评估。

自定义实现

```python
best_method = ""
best_score = 0.0
best_k = -1
for method in ["uniform", "distance"]:
		for k in range(1, 11):
  			knn_clf = KNeighborsClassifier(n_neighbors=k)
    		knn_clf.fit(X_train, y_train)
    		score = knn_clf.score(X_test, y_test)
    		if score > best_score:
      			best_k = k
      			best_score = score
            best_method = method
print("best_k = ", best_k)
print("best_score = ", best_score)
print("best_method = ", best_method)
```

sklearn实现

```python
# 对估计器的指定参数值进行详尽搜索
sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)

# 输入
estimator：估计器对象
param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
cv：指定几折交叉验证
# 方法
fit：输入训练数据
score：准确率
# 属性
best_score_:最好结果
best_estimator_：最好的参数模型
cv_results_:交叉验证的结果
```

示例

```python
# 1.导入所需要的包
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

# 2.载入数据
data = pd.read_csv('./FBlocation/train.csv')
print(len(data))
# 3.缩小数据范围
data = data.query("x > 1 & x < 1.25 & y >3 &y < 3.25")
print(len(data))
# 4.时间特征抽取
# 将时间戳转换为日期
time_value = pd.to_datetime(data["time"], unit="s")
# 将时间转换为DatetimeIndex
date_time_index = pd.DatetimeIndex(time_value)
data["hour"] = date_time_index.hour
data["month"] = date_time_index.month
data["dayofweek"] = date_time_index.dayofweek
# 5.删除掉入住率比较低的样本
# 分组聚合 以place_id分组，count计数，小于3，筛选掉
place_count = data.groupby("place_id").aggregate(np.count_nonzero)
# print(place_count)
#            row_id      x      y  accuracy  time  hour  month  dayofweek
# place_id
# 1009781224     219  219.0  219.0       219   219   216    219        200
# 所有入住次数大于3的结果，数据并不是原始数据，而只是一个统计数据
result = place_count[place_count["row_id"] > 3].reset_index()
# 从原始数据中选择place_id在result中的样本
data = data[data["place_id"].isin(result["place_id"])]
# 6.特征选择
# 特征值
x = data.drop(["row_id", "time", "place_id"], axis=1)
# 目标值
y = data["place_id"]
# 7.分割数据集
x_train, x_test, y_train, y_test = train_test_split(x, y)
# 8.对数据集进标准化
ss = StandardScaler()
# 对特征值进行标准化
x_train = ss.fit_transform(x_train) 
# 对测试集的特征值标准化
x_test = ss.transform(x_test)  # 按照原来训练集的平均值做标准化，统一数据转换标准
# 9.KNeighborsClassifiler训练模型
knn = KNeighborsClassifier(n_neighbors=3)
# 网格搜索与交叉验证
params = {"n_neighbors": [1, 3, 5]}
gscv = GridSearchCV(estimator=knn, param_grid=params, cv=2)
gscv.fit(x_train, y_train)
print(gscv.best_params_)
print(gscv.best_estimator_)
print(gscv.best_score_)
print(gscv.cv_results_)
```

## 模型比较理论

最大似然估计

奥卡姆剃刀

## 评价

### 回归

- 均方误差(Mean Squared Error)

$$
MSE = \frac{\sum_{i=1}^m{(y_{test}^{(i)}-\hat{y}_{test}^{(i)})^2}}{m}
$$

- 根均方误差(Root Mean Squared Error)

```
可降低量纲的影响
```

$$
RMSE = \sqrt{MSE}
$$

- 平均绝对误差(Mean Absolute Error)

$$
MAE = \frac{\arrowvert y_{test}^{(i)}-\hat{y}_{test}^{(i)}\arrowvert}{m}
$$
- R方

```
可使用于不同量纲(业务)情况下比较,小于等于1，
越大表示模型越好，
等于基准模型时为0，
小于0时表示训练的模型还不如基准模型，可能不存在任何线性关系
```

$$
R^2 = 1 - \frac{SS_{redidual}}{SS_{total}}=1 - \frac{\sum_i{(\hat{y}^{(i)}-y^{(i)})^2}}{\sum_i{(\bar{y}^{(i)}-y^{(i)})^2}} =1 - \frac{(\sum_{i=1}^m{(\hat{y}^{(i)}-y^{(i)})^2})/m}{(\sum_{i=1}^m{(\bar{y}^{(i)}-y^{(i)})^2})/m}= 1 - \frac{MSE(\hat{y}, y)}{Var(y)}
$$

自定义实现

```python
# metrics.py
import numpy as np
from math import sqrt


def accuracy_score(y_true, y_predict):
    """计算y_true和y_predict之间的准确率"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(y_true == y_predict) / len(y_true)


def mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的MSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum((y_true - y_predict)**2) / len(y_true)


def root_mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""

    return sqrt(mean_squared_error(y_true, y_predict))


def mean_absolute_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)


def r2_score(y_true, y_predict):
    """计算y_true和y_predict之间的R Square"""

    return 1 - mean_squared_error(y_true, y_predict)/np.var(y_true)

```

sklearn实现

```python
# 均方误差
from sklearn.metrics import mean_squared_error


# 根均方误差
sqrt(mean_squared_error())

# 平均绝对误差
from sklearn.metrics import mean_absolute_error


# R方
from sklearn.metrics import r2_score
```

### 分类

- 准确度

$$
accuracy = \frac{\sum_{i=1}^m{(y_{test}^{(i)}==\hat{y}_{test}^{(i)})}}{m}
$$

对于极度偏斜(Skewed Data)的数据，分类准确度时不够的

- 精准率/召回率

1为关注的事件(Positive)，0为对立事件(Negative)

| 真实\预测 | 0    | 1    |
| --------- | ---- | ---- |
| 0         | TN   | FP   |
| 1         | FN   | TP   |

**精准率**：表示的是预测为正的样本中有多少是真正的正样本。有两种可能：把正类预测为正类(TP)，是把负类预测为正类(FP)。
$$
precision = \frac{TP}{TP+FP}
$$
**召回率**：表示的是样本中的正例有多少被预测正确了。有两种可能：把原来的正类预测成正类(TP)，把原来的正类预测为负类(FN)。
$$
recall = \frac{TP}{TP+FN}
$$
- F1 Score

反应了模型的稳健性，是precision和recall的调和平均值
$$
\frac{1}{F1} = \frac{1}{2}(\frac{1}{precision}+\frac{1}{recall}) 
$$
得到
$$
\\
F1 = \frac{2\cdot{precision}\cdot{recall}}{precision+recall}
$$

- 精准率和召回率平衡(PR曲线)

随着阈值的变化，精准率和召回率呈现反向变化的情况，故需要寻找之间的平衡，可以确定较好的阈值来分类

mAP是为结局P、R、F-measure的单点值局限性的

召回率*准确率的面积越大，则map越大，模型越好

- ROC曲线

Receiver Operation Characteristic Curve

描述TPR和FPR之间的关系， 可用于比较不同的模型好坏
$$
TPR = Recall = \frac{TP}{TP+FN} \\
FPR = \frac{FP}{TN+FP}
$$

- 实现

自实现

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

digits = datasets.load_digits()
X = digitis.data
y = digitis.target.copy()
# 对数据手动偏斜
y[digits.target==9] = 1
y[digits.target!=9] = 0

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

def TN(y_true, y_predict):
  	assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0)&(y_predict == 0))

def FP(y_true, y_predict):
  	assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0)&(y_predict == 1))
  
def FN(y_true, y_predict):
  	assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1)&(y_predict == 0))
  
def TP(y_true, y_predict):
  	assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1)&(y_predict == 1))
  
def confusion_matrix(y_true, y_predict):
		return np.array([
      	[TN(y_test, y_predict), FP(y_test, y_predict)],
      	[FN(y_test, y_predict), TP(y_test, y_predict)]
    ])

def precision_score(y_true, y_predict):
  	tp = TP(y_test, y_predict)
    fp = FP(y_test, y_log_predict)
    try:
      	return tp / (tp + fp)
    except:
      	retunr 0.0
  
def recall_score(y_true, y_predict):
  	tp = TP(y_test, y_predict)
    fn = FN(y_test, y_predict)
    try:
    		return tp / (tp + fn)
    except:
      	return 0.0
      
def f1_score(y_true, y_predict):
  	precision = precision_score(y_true, y_predict)
    recall = recall_score(y_true, y_predict)
  	try:
      	return 2 * precision*recall / (precision + recall)
    except:
      	return 0.0
      
def TPR(y_true, y_predict):
  	tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
      	return tp / (tp + fn)
    except:
      	return 0.

def FPR(y_true, y_predict):
  	fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    try:
      	return fp / (fp + tn)
    except:
       	return 0.
  
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)  # 准确度

y_log_predict = log_reg.predict(X_test)

TN(y_test, y_log_predict)
FP(y_test, y_log_predict)
FN(y_test, y_log_predict)
TP(y_test, y_log_predict)
confusion_matrix(y_test, y_log_predict)  # 混淆矩阵
precision = precision_score(y_test, y_log_predict)  # 精确度
print(precision)
recall = recall_score(y_test, y_log_predict)  # 召回率
print(recall)
f1_score = f1_score(y_test, y_log_predict)  # f1
print(f1_score)

# 决策函数返回score值，>0则预测为1，<0则预测为0
decision_scores = log_reg.decision_function(X_test)

# 使用5作为预测结果阈值
y_predict_2 = np.array(decision_scores >= 5, dtype='int')
confusion_matrix(y_test, y_predict_2)
precision = precision_score(y_test, y_predict_2)
recall = recall_score(y_test, y_predict_2)
f1_score = f1_score(y_test, y_predict_2)

# 自定义绘图precision-recall曲线
precisions = []
recalls = []
thresholds = np.arrange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
  	y_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, y_predict))
    recalls.append(recall_score(y_test, y_predict))
    
plt.plot(thresholds, precisions)
plt.plot(thresholds, recalls)
plt.show()
plt.plot(precisions, recalls)
plt.show()

# ROC曲线
fprs = []
tprs = []
thresholds = np.arrange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
  	y_predict = np.array(decision_scores >= threshold, dtype='int')
    fprs.append(FPR(y_test, y_predict))
    tprs.append(TPR(y_test, y_predict))
    
plt.plot(fprs, tprs)
plt.show()
```

> sklearn实现

API

```python
sklearn.metrics.confusion_matrix(y_true, y_pred,)
# 参数
y_true：真实目标值
y_pred：估计器预测目标值


sklearn.metrics.classification_report(y_true, y_pred, target_names=None)
# 参数
y_true：真实目标值
y_pred：估计器预测目标值
target_names：目标类别名称
# 返回
每个类别精确率与召回率
```

示例

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

digits = datasets.load_digits()
X = digitis.data
y = digitis.target.copy()
# 对数据手动偏斜
y[digits.target==9] = 1
y[digits.target!=9] = 0

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)  # 准确度

y_log_predict = log_reg.predict(X_test)

confusion_matrix(y_test, y_log_predict)  # 混淆矩阵
precision = precision_score(y_test, y_log_predict)  # 精确度
recall = recall_score(y_test, y_log_predict)  # 召回率
f1_score = f1_score(y_test, y_log_predict)  # f1

# sklearn实现precision-recall曲线
precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()
plt.plot(precisions, recalls)
plt.show()

# ROC
fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()

roc_auc_score(y_test, decision_scores)
```

多分类

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

digits = datasets.load_digits()
X = digits.data
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)

y_predict = log_reg.predict(X_test)

cfm = confusion_matrix(y_test, y_predict)
print(cfm)
plt.matshow(cfm, cmap=plt.cm.gray)
plt.show()

row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)
print(err_matrix)
plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()  # 越亮的地方即错误最大的地方
```

### 聚类

- 轮廓系数

Silhouette Coefficient
$$
s_i = \frac{b_i-a_i}{max\{a_i,b_i\}}\\
s_i = \begin{cases}
			 1-\frac{a_i}{b_i},&a_i<b_i\\
			 0, & a_i=b_i\\
			 \frac{b_i}{a_i}-1, & a_i>b_i
			 \end{cases}
$$
计算样本i到同簇其他样本的平均距离$a_i$。$a_i$越小，说明样本i越应该被聚类到该簇。将$a_i$称为样本i的簇内不相似度。

计算样本i到其他某簇$C_j$的所有样本的平均距离$b_{ij}$，称为样本i与簇$C_j$的不相似度。定义为样本i的簇间不相似度：$b_i=min\{b_{i1},b_{i2},…,b_{ik}\}$

$s_i$接近1，则说明样本i聚类合理

$s_i$接近-1，则说明样本i更应该分类到另外的簇

$s_i$近似为0，则说明样本i在两个簇的边界上

sklearn实现

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

labels = KMeans(n_clusters=3).fit(X).labels_
score = silhouette_score(X, labels)
```

## 拟合泛化

过拟合

```
算法所训练的模型过多地表达了数据间的噪音关系
```

欠拟合

```
算法所训练的模型不能完整表述数据间的关系
```

数据集分为测试数据集和训练数据集，可以提高模型的泛化能力

## 学习曲线

随着训练样本的逐渐增多，算法训练出的模型的表现能力

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler


np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * X ** 2 + x * 2 + np.random.normal(0, 1, size = 100)

plt.scatter(x, y)
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
print(X_train.shape)

train_score = []
test_score = []
for in in range(1, 76):
  	lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    y_train_predict = lin_reg.predict(X_tain[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
    
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

# 封装函数
def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
		train_score = []
		test_score = []
		for in in range(1, 76):
    		algo.fit(X_train[:i], y_train[:i])
    		y_train_predict = algo.predict(X_tain[:i])
    		train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    		y_test_predict = algo.predict(X_test)
    		test_score.append(mean_squared_error(y_test, y_test_predict))
    
		plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
		plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
		plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
		plt.show()
# 线性回归调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)
# 多项式回归调用
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)
# 过拟合调用
poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)
```

##  偏差方差均衡

Bias Variance Trade off

模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差

常见现象

```
偏差和方差通常是矛盾的，降低偏差，会提高方差；降低方差，会提高偏差
```

偏差产生原因

```
对问题本身的假设不正确
如非线性回归使用线性回归，造成欠拟合
如特征选取错误
```

方差产生的原因

```
使用的模型太复杂
如高阶多项式回归，造成过拟合
```

算法原因

```
有一些算法天生是高方差的算法，如knn
非参数学习通常都是高方差算法，因为不对数据进行任何假设

有一些算法天生是高偏差的算法，如线性回归
参数学习通常都是高偏差算法，因为对数据具有极强的假设

大多数算法具有相应的参数，可以调整偏差和方差
如knn中的k,线性回归中使用多项式回归
```

机器学习算法的主要挑战，是方差

解决防擦好的常用手段

```
1. 降低模型复杂度
2. 减少数据维度，降噪
3. 增加样本数
4. 使用验证集
5. 模型正则化
```

## 模型正则化

Regularization，限制参数的大小，可以提高模型泛化

- 原理

在线性回归的损失函数
$$
\sum_{i=1}^m{(y^{(i)}-\theta_0-\theta_1X_1^{(i)}-\ldots-\theta_nX_n^{(i)})^2}
$$
也就是
$$
J(\theta) = MSE(y, \hat{y}; \theta)
$$
加入模型正则化
$$
J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\frac{1}{2}\sum_{i=1}^n{\theta_i^2}
$$

- L1、L2、L0正则

$L_p$范数
$$
\Arrowvert{x}\Arrowvert_p = (\sum_{i=1}^n{\arrowvert{x_i}\arrowvert^p})^{\frac{1}{p}}
$$
Ridge添加正则化部分
$$
\sum_{i=1}^n{\theta_i^2}
$$
被称为L2正则项

LASSO添加正则化部分
$$
\sum_{i=1}^n{\arrowvert{\theta_i}\arrowvert}
$$
被称为L1正则项
$$
J(\theta) = MSE(y, \hat{y}; \theta) + min\{number-of-non-zero-\theta\}
$$
使$J(\theta)$中$\theta$个数尽可能少，即为L0正则项

对于L0正则的优化是一个NP难的问题，通常使用L1取代L0

- 弹性网(Elastic Net)

$$
J(\theta) = MSE(y, \hat{y}; \theta) + r\alpha\sum_{i=1}^n\arrowvert{\theta_i}\arrowvert + \frac{1-r}{2}\alpha\sum_{i=1}^n{\theta_i^2}
$$

- 实现

回归算法中使用模型正则化来提高泛化能力的有岭回归、拉索回归等

- 应用

计算量可接受时，优先尝试岭回归

计算量过大时，优先使用弹性网

## 模型保存与加载

svm模型

```python
# 保存模型
import pickle
with open("./svm.model", "wb") as f:
    pickle.dump(svc, f)

# 加载模型
with open("./svm.model", "rb") as f:
    svc = pickle.load(f)
```

