# 模型相关

## 数据集                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

离散型数据：由于记录不同类别个体的数目所获得的数据，又称计数数据，不能再细分，也不能提高精度

连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，这类数据通常是非整数，含有小数部分

- 可用数据集

Kaggle网址：<https://www.kaggle.com/datasets>

UCI数据集网址：<http://archive.ics.uci.edu/ml/>

scikit-learn网址：[http://scikit-learn.org/stable/datasets/index.html#datasets](http://scikit-learn.org/stable/datasets/index.html)

- 数据集划分

训练数据：用于训练，构建模型

测试数据：用于校验，测试模型是否有效

划分比例：`70:30, 80:20, 75:30`

作用：可以测试模型的泛化能力

```python
# model_selection.py
import numpy as np


def train_test_split(X, y, test_ratio=0.2, seed=None):
    """将数据 X 和 y 按照test_ratio分割成X_train, X_test, y_train, y_test"""
    assert X.shape[0] == y.shape[0], \
        "the size of X must be equal to the size of y"
    assert 0.0 <= test_ratio <= 1.0, \
        "test_ration must be valid"

    if seed:
        np.random.seed(seed)

    shuffled_indexes = np.random.permutation(len(X))

    test_size = int(len(X) * test_ratio)
    test_indexes = shuffled_indexes[:test_size]
    train_indexes = shuffled_indexes[test_size:]

    X_train = X[train_indexes]
    y_train = y[train_indexes]

    X_test = X[test_indexes]
    y_test = y[test_indexes]

    return X_train, X_test, y_train, y_test
```

## 交叉验证

Cross Validation

训练数据：训练模型使用的数据集

验证数据：调整超参数使用的数据集

测试数据：作为衡量最终模型性能的数据集

如果对所有的数据进行模型训练，则可能造成过拟合，上线后无法进行模型调整，故需要测试数据集

如是仅仅用测试数据集对模型进行验证调整，则可能造成针对特定测试数据集过拟合，故在数据中随机使用部分数据作为验证数据集

有了验证数据，则可以在数据训练时采用验证数据进行验证来调整超参数，之后再用测试数据集进行测试，但是随机带来新的问题，验证数据可能存在极端情况，会造成对验证数据集的过拟合，故需要交叉验证

缺点

```
把训练集分成k份，成为k-folds cross validation，每次训练K个模型，整体性能慢了k倍
```

留一法

```
LOO-CV
有m个样本，把训练数据集分成m份， 称为留一法，Leave-One-Out Cross Validation

优点：
完全不受随机的影响，最接近模型真正的性能指标
缺点：
计算量大
```

示例

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cros_val_score
from sklearn.model_selection import GridSearchCV

digits = datasets.load_digits()
X = digits.data
y = digits.target

# 测试数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)

# 手动循环超参数验证
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

# 交叉验证
knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
# cross_val_score(knn_clf, X_train, y_train, cv=5)  # 指定训练集分割份数
best_score, best_p, best_k = 0, 0, 0
for k in range(2, 11):
  	for p in range(1, 6):
      	knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.mean(scores)
        if score > best_score:
          	best_score = score
            best_p = p
            best_k = k
print("best_score", best_score)
print("best_p", best_p)
print("bset_k", best_k)

best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2 p=2)
best_knn_clf.fit(X_train, y_train)
score = best_knn_clf.score(X_test, y_test)
print(score)

# 网格搜索
param_grid = [
  {
    'weights': ['distance'],
    'n_neighbors': [ i for i in range(2, 11)],
    'p': [i for i in range(1, 6)]
  }
]
grid_search = GridSearchCV(knn_clf, param_grid, verbose=1)
# GridSearchCV(knn_clf, param_grid, verbose=1, cv=5)  # 指定训练集分割份数
grid_search.fit(X_train, y_train)
print(grid_search.best_score_)
print(grid_search.best_params_)
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
```







## 评价

### KNN

准确度
$$
accuracy = \frac{\sum_{i=1}^m{(y_{test}^{(i)}==\hat{y}_{test}^{(i)})}}{m}
$$

### 线性回归

均方误差(Mean Squared Error)
$$
MSE = \frac{\sum_{i=1}^m{(y_{test}^{(i)}-\hat{y}_{test}^{(i)})^2}}{m}
$$


根均方误差(Root Mean Squared Error)

```
可降低量纲的影响
```

$$
RMSE = \sqrt{MSE}
$$

平均绝对误差(Mean Absolute Error)
$$
MAE = \frac{\arrowvert y_{test}^{(i)}-\hat{y}_{test}^{(i)}\arrowvert}{m}
$$
R方

```
可使用于不同量纲(业务)情况下比较,小于等于1，
越大表示模型越好，
等于基准模型时为0，
小于0时表示训练的模型还不如基准模型，可能不存在任何线性关系
```

$$
R^2 = 1 - \frac{SS_{redidual}}{SS_{total}}=1 - \frac{\sum_i{(\hat{y}^{(i)}-y^{(i)})^2}}{\sum_i{(\bar{y}^{(i)}-y^{(i)})^2}} =1 - \frac{(\sum_{i=1}^m{(\hat{y}^{(i)}-y^{(i)})^2})/m}{(\sum_{i=1}^m{(\bar{y}^{(i)}-y^{(i)})^2})/m}= 1 - \frac{MSE(\hat{y}, y)}{Var(y)}
$$



示例

```python
# metrics.py
import numpy as np
from math import sqrt


def accuracy_score(y_true, y_predict):
    """计算y_true和y_predict之间的准确率"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(y_true == y_predict) / len(y_true)


def mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的MSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum((y_true - y_predict)**2) / len(y_true)


def root_mean_squared_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""

    return sqrt(mean_squared_error(y_true, y_predict))


def mean_absolute_error(y_true, y_predict):
    """计算y_true和y_predict之间的RMSE"""
    assert len(y_true) == len(y_predict), \
        "the size of y_true must be equal to the size of y_predict"

    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)


def r2_score(y_true, y_predict):
    """计算y_true和y_predict之间的R Square"""

    return 1 - mean_squared_error(y_true, y_predict)/np.var(y_true)

```

## 超参数

模型参数：算法过程中学习的参数

超参数：在算法运行前需要决定的参数

超参数选择

```
1. 领域知识
2. 经验数值
3. 实验搜索
```

实验搜索

```python
best_method = ""
best_score = 0.0
best_k = -1
for method in ["uniform", "distance"]:
		for k in range(1, 11):
  			knn_clf = KNeighborsClassifier(n_neighbors=k)
    		knn_clf.fit(X_train, y_train)
    		score = knn_clf.score(X_test, y_test)
    		if score > best_score:
      			best_k = k
      			best_score = score
            best_method = method
print("best_k = ", best_k)
print("best_score = ", best_score)
print("best_method = ", best_method)
```

## 拟合泛化

过拟合

```
算法所训练的模型过多地表达了数据间的噪音关系
```

欠拟合

```
算法所训练的模型不能完整表述数据间的关系
```

数据集分为测试数据集和训练数据集，可以提高模型的泛化能力

## 学习曲线

随着训练样本的逐渐增多，算法训练出的模型的表现能力

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler


np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * X ** 2 + x * 2 + np.random.normal(0, 1, size = 100)

plt.scatter(x, y)
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
print(X_train.shape)

train_score = []
test_score = []
for in in range(1, 76):
  	lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    y_train_predict = lin_reg.predict(X_tain[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
    
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

# 封装函数
def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
		train_score = []
		test_score = []
		for in in range(1, 76):
    		algo.fit(X_train[:i], y_train[:i])
    		y_train_predict = algo.predict(X_tain[:i])
    		train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    		y_test_predict = algo.predict(X_test)
    		test_score.append(mean_squared_error(y_test, y_test_predict))
    
		plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
		plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
		plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
		plt.show()
# 线性回归调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)
# 多项式回归调用
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)
# 过拟合调用
poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)
```

##  偏差方差均衡

Bias Variance Trade off

模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差

常见现象

```
偏差和方差通常是矛盾的，降低偏差，会提高方差；降低方差，会提高偏差
```

偏差产生原因

```
对问题本身的假设不正确
如非线性回归使用线性回归，造成欠拟合
如特征选取错误
```

方差产生的原因

```
使用的模型太复杂
如高阶多项式回归，造成过拟合
```

算法原因

```
有一些算法天生是高方差的算法，如knn
非参数学习通常都是高方差算法，因为不对数据进行任何假设

有一些算法天生是高偏差的算法，如线性回归
参数学习通常都是高偏差算法，因为对数据具有极强的假设

大多数算法具有相应的参数，可以调整偏差和方差
如knn中的k,线性回归中使用多项式回归
```

机器学习算法的主要挑战，是方差

解决防擦好的常用手段

```
1. 降低模型复杂度
2. 减少数据维度，降噪
3. 增加样本数
4. 使用验证集
5. 模型正则化
```

## 模型正则化

Regularization，限制参数的大小，可以提高模型泛化

- 原理

在线性回归的损失函数
$$
\sum_{i=1}^m{(y^{(i)}-\theta_0-\theta_1X_1^{(i)}-\ldots-\theta_nX_n^{(i)})^2}
$$
也就是
$$
J(\theta) = MSE(y, \hat{y}; \theta)
$$
加入模型正则化
$$
J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\frac{1}{2}\sum_{i=1}^n{\theta_i^2}
$$

- L1、L2、L0正则

$L_p$范数
$$
\Arrowvert{x}\Arrowvert_p = (\sum_{i=1}^n{\arrowvert{x_i}\arrowvert^p})^{\frac{1}{p}}
$$
Ridge添加正则化部分
$$
\sum_{i=1}^n{\theta_i^2}
$$
被称为L2正则项

LASSO添加正则化部分
$$
\sum_{i=1}^n{\arrowvert{\theta_i}\arrowvert}
$$
被称为L1正则项
$$
J(\theta) = MSE(y, \hat{y}; \theta) + min\{number-of-non-zero-\theta\}
$$
使$J(\theta)$中$\theta$个数尽可能少，即为L0正则项

对于L0正则的优化是一个NP难的问题，通常使用L1取代L0

- 弹性网(Elastic Net)

$$
J(\theta) = MSE(y, \hat{y}; \theta) + r\alpha\sum_{i=1}^n\arrowvert{\theta_i}\arrowvert + \frac{1-r}{2}\alpha\sum_{i=1}^n{\theta_i^2}
$$

- 实现

回归算法中使用模型正则化来提高泛化能力的有岭回归、拉索回归等

- 应用

计算量可接受时，优先尝试岭回归

计算量过大时，优先使用弹性网



