# 梯度

## 梯度下降

Gradient Descent

不是一个机器学习算法

是一种基于搜索的最优化方法

作用：最小化一个损失函数
$$
-\eta\frac{\mathrm{d}J}{\mathrm{d}\theta}
$$
其中$\eta$

```
为学习率(learning rate)
取值影响获得最优解的速度，甚至收敛性
太小，速度过低
太大，甚至导致不收敛
是梯度下降法的一个超参数
```

注意：并不是所有函数都有唯一的极值点

解决方案

```
1. 多次运行，随机初始化点
2. 梯度下降法的初始点也是一个超参数
```

### 模拟梯度下降

```python
import numpy as np
import matplotlib.pyplot as plt


plot_x = np.linespace(-1, 6, 141)  # theta
plot_y = (plot_x-2.5)**2-1  # 损失函数

def dJ(theta):
  	return 2*(theta - 2.5)
  
def J(theta):
  	try:
  		return (theta-2.5)**2 - 1
    except:
      	return float('inf')  # 极大值

def gradient_descent(initial_theta, eta, n_iters=1e4, epsilon=1e-8):
  	theta = initial_theta
    theta_history.append(initial_theta)
    i_iter = 0
    while i_iter < n_iters:  # 避免无止境
      	gradient = dJ(theta)
    		last_theta = theta
    		theta = theta - eta * gradient
    		theta_history.append(theta)
    
    		if(abs(J(theta) - J(last_theta)) < epsilon)：
    				break
				i_iter += 1
        
    print(theta)
    print(J(theta))
        
def plot_theta_history():
  	plt.plot(plot_x, J(plot_x))
		plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
		plt.show()
    
eta = 0.01  # 可使用不同值来验证
theta_history = []
gradient_descent(0., eta)
plot_theta_history()
```

### 批量梯度下降

>  Batch Gradient Descent

- 算法流程

每次迭代遍历数据集时，保存每组训练数据对应的梯度增量。遍历结束后，计算数据集的梯度增量之和，最后调整所有模型参数。不断迭代后，BGD算法能收敛于全局最优解。

伪代码

```python
Repeate until convergence {
	w_j := w_j - \alpha\sum_{i=1}^{m}{\delta w_j^(i)}    (for every j)
}

# m为训练数据集规模；\alpha是学习率，w是待优化的参数，\delta w为参数w的梯度增量
```

- 缺点
```
1. 由于BGD算法每次迭代调整一次模型参数，无疑阻碍了迭代训练；
2. 同时BGD算法每次迭代需要保存每组数据对应的梯度增量，那么当训练数据规模较大时，会带来很大的空间 开销，甚至可能导致系统内存溢出。
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ(theta, X_b, y):
  	res = enp.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
      	res[i] = (X_b.dot(theta) - y).doct(X-b[:,i])
    return res * 2 / len(X_b)
  
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta[0], theta[1:])
```

### 随机梯度下降

> Stochastic Gradient Descent

BGD算法虽然收敛于 全局最优解，但是收敛速度慢，适用性不强，为了快速训练模型，SGD可以替代。

- 算法流程

在一次迭代训练中，依次遍历数据集中的每组数据，利用每组数据对应的梯度增量来调整模型参数。也就是说，对于一个含有m组数据的数据集，在每次迭代训练中，必须调整模型参数m次。同时在实际运用中，首先会将训练数据集随机打乱

伪代码

```python
Loop {
	for i = 1 to m {
		w_j := w_j - {\alpha/n}{\delta w_j^(i)}    (for every j)
	}

}
```

- 优缺点

```python
#  优点
1. 跳出局部最优解
2. 更快的运行速度

# 缺点
1.SGD属于贪心算法，最终求得的是次优解，而不是全局最优解
每次迭代训练中，SGD算法相比BGD算法虽频繁地调整超参数，加快了收敛速度，但SGD算法也会出现偏差，每次迭代只用一组数据进行参数调整，而一组数据的梯度不能代表整体数据集的梯度方向，因而不可能都沿着全局最优解方向，故可能会陷入局部最优解。换句话说，SGD理论上不能收敛到全局最优解，而是在局部最优解的附近邻域内振荡。
```

实现

```python
import numpy as np
import matplotlib.pyplot as plt

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.
  
def sgd(X_b, y, initial_theta, n_iters):
  	t0 = 5
    t1 = 50
    def learning_rate(t):
      	return t0 / (t + t1)
    theta = initial_theta
    for cur_iter in range(n_iters):
      	rand_i = np.random.randint(len(X_b))
        grdient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * grdient
    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)
print(theta[0], theta[1:])
```

### 小批量梯度下降

> Mini-Batch Gradient Descent

MBGD算法是BGD算法和SGD算法的折中。

- 算法流程

首先将训练集随机打乱，并划分成若干均等小样本；然后每次迭代后遍历每个小样本，计算小批量样本的梯度增量平均值，并根据计算的平均值调整超参数。那么，在小批量样本规模足够大时，小批量样本梯度向量的平均值在误差允许范围内近似等于全体训练样本梯度增量的平均值

伪代码

```python
m = Data Size
n = Mini Batch Size
Repeat until convergence{
    for i = 1 to m/n:
    	w_j := w_j - {\alpha/n}\sum_{i=1}^{m}{\delta w_j^(i)}    (for every j)
    	
}
```

- 优缺点

```
MBGD算法兼顾了BGD算法和SGD算法的优点，虽然需要开辟较小的空间来保存小样本的梯度增量以计算平均值，但是也保留了略慢于SGD算法的收敛速度。
MGBD算法相比BGD算法加快了收敛速度，相比于SGD算法降低了迭代训练中陷入局部最优解的风险。
```

## 梯度调试

对于曲线上某点的导数，可近似认为
$$
\frac{\mathrm{d}J}{\mathrm{d}\theta} = \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}
$$
则
$$
\theta = (\theta_0, \theta_1,\cdots ,\theta_n)
$$

$$
\frac{\partial{J}}{\partial{\theta}} = (\frac{\partial{J}}{\partial{\theta_0}},\frac{\partial{J}}{\partial{\theta_1}},\ldots,\frac{\partial{J}}{\partial{\theta_n}})
$$

有
$$
\theta_1^+ = (\theta_0, \theta_1+\varepsilon,\cdots,\theta_n)
$$

$$
\theta_1^- = (\theta_0, \theta_1+\varepsilon,\cdots,\theta_n)
$$

$$
\frac{\partial{J}}{\partial{\theta_1}} = \frac{J(\theta_1^{+})-J(\theta_1^{-})}{2\varepsilon}
$$



虽然算法复杂度高，但是可用于小批量数据验证梯度下降算法

步骤

```
1. 使用debug模式获得正确解
2. 进行数学求解
3. 比对验证两种结果
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.random(size=(1000, 10))
true_theta = np.arrange(1, 12, dtype=float)
X_b = np.hstack([np.ones((len(X), 1)), X])
y = X_b.dot(true_theta) + np.random.normal(size=1000)

def J(theta, X_b, y):  # 损失函数
  	try:
      	return np.sum((y-X_b.dot(theta)**2)/len(X_b))
    except:
      	return float('inf')
      
def dJ_math(theta, X_b, y):
  	"""数学向量化求导，特定情况"""
		return X_b.dot(X_b.dot(theta) - y) * 2. / len(y)
  
def dJ_debug(theta, X_b, y, epsilon=0.01):
  	"""近似求导，通用"""
  	res = np.empty(len(theta))
    for i in range(len(theta)):
      	theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y))/(2*epsilon)
    return res
  
def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
		"""批量梯度下降法"""
    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
 
X_b = np.hstack([np.ones((len(X), 1)), X])
init_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(dj_debug, X_b, y, initial_theta, eta)
theta = gradient_descent(dj_math, X_b, y, initial_theta, eta)
```

## 梯度上升

最大一个效用函数
$$
+\eta\frac{\mathrm{d}J}{\mathrm{d}\theta}
$$
