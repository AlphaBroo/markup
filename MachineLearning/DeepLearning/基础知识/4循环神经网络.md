# 循环神经网络

**循环(递归)神经网络**（**RNN**）是神经网络的一种。RNN将状态在自身网络中循环传递，可以接受时间序列结构输入。

- 类型

<img src="images/RNN结构.png" alt="RNN结构" style="zoom:50%;" />

一对一：固定的输入到输出，**如图像分类**
一对多：固定的输入到序列输出，**如图像的文字描述**
多对一：序列输入到输出，**如情感分析，分类正面负面情绪**
多对多：序列输入到序列的输出，**如机器翻译,称之为编解码网络**
同步多对多：同步序列输入到同步输出，**如文本生成，视频每一帧的分类，也称之为序列生成**

## 序列模型

定义

```
通常在自然语言、音频、视频以及其它序列数据的模型。
```

类型

```
语音识别，输入一段语音输出对应的文字
情感分类，输入一段表示用户情感的文字，输出情感类别或者评分
机器翻译，两种语言的互相翻译
```

CNN效果不好的原因

```
序列数据前后之间是有很强的关联性
序列数据的输入输出长度不固定
```

## 基础循环网络

### 概述

<img src="images/基础RNN结构.png" alt="基础RNN结构" style="zoom:50%;" />

- 元素

$x_t$ :表示每一个时刻的输入

$o_t$:表示每一个时刻的输出

$s_t$:表示每一个隐层的输出

中间的小圆圈代表隐藏层的一个unit(单元)

所有单元的**参数共享**

- 通用公式

$$
s_0 = 0 \\
s_t = g_1(U_{x_t}+W_{x_{t-1}}+b_a)\\
o_t = g_2(V_{s_t}+b_y)
$$

$g_1,g_2$:表示激活函数，$g_1$:tanh/relu, $g_2$:sigmoid、softmax

循环神经网络的输出值$o_t$，是受前面历次输入值$x_{t-1},x_{t},x_{t+1}$影响

### 生成案例

通常对于整个序列给一个开始和结束标志，start,end标志。

- s 我 昨天 上学 迟到 了 e

输入到网络当中的是一个个的分词结果，每一个词的输入是一个时刻。

<img src="images/序列例子.png" alt="序列例子" style="zoom:50%;" />

### 词的表示

为了能够让整个网络能够理解我们的输入（英文/中文等），需要将词进行用向量表示。

- 建立一个包含所有序列词的词典包含（**开始和标志的两个特殊词，以及没有出现过的词用等**），每个词在词典里面有一个唯一的编号。
- 任意一个词都可以用一个N维的one-hot向量来表示。其中，N是词典中包含的词的个数

<img src="images/词向量表示.png" alt="词向量表示" style="zoom:50%;" />

我们就得到了一个高维、**稀疏**的向量（稀疏是指绝大部分元素的值都是0）

### softmax输出

RNN这种模型，**每一个时刻的输出**是下一个最可能的词，可以用概率表示，总长度为词的总数长度

<img src="images/输出表示.png" alt="输出表示" style="zoom:50%;" />

每一个时刻的输出 $s_t$ 都是词的总数长度，接上softmax回归即可

### 矩阵运算

对于网络当中某一时刻的公式中
$$
\mathrm{s}_t=relu(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\
o_{t} = softmax(V{s_t})
$$
<img src="images/向量化表示.png" alt="向量化表示" style="zoom:50%;" />

1、形状表示:

[n, m]x [m, 1] +[n, n]x [n, 1] = [n, 1]

则**矩阵U的维度是n x m，矩阵W的维度是n x n**

m：词的个数，n：为输出s的维度

注:此步骤可以简化：[u,w] x [$\frac{x}{s}$ ] = [n, n+m] x [n +m, 1] = [n, 1]

2、形状表示：[m, n] x [n, 1] = [m, 1]

**矩阵V维度：[m, n]**

**总结：其中的n是可以人为去进行设置。**

### 交叉熵损失

总损失定义：

一整个序列（一个句子）作为一个训练实例，总误差就是各个时刻词的误差之和。
$$
E_{t}(y_{t},\hat{y_{t}}) = -y_{t}log(\hat{y_{t}})\\

E(y,\hat{y}) = \sum_{t}E_{t}(y_{t},\hat{y_{t}})=-\sum_{t}y_{t}log(\hat{y_{t}})
$$

在这里，$ y_t$ 是时刻 t 上正确的词，$ \hat y_{t}$是预测出来的词

### 完整计算流程

<img src="images/RNN 完整流程.png" alt="RNN 完整流程" style="zoom:50%;" />
$$
\mathrm{s}^t=tanh(U\mathrm{x}^t+W\mathrm{s}^{t-1}+b_a)\\
o^{t} = softmax(V{s^t + b_y})
$$

### 时序反向传播算法

对于RNN来说有一个时间概念，需要把梯度沿时间通道传播的 BP 算法，所以称为Back Propagation Through Time-BPTT

<img src="images/损失1.png" alt="损失1" style="zoom:50%;" />

我们的目标是计算误差关于**参数U、V和W以及两个偏置bx,by的梯度**，然后使用梯度下降法学习出好的参数。由于这三组参数是共享的，我们需要将一个训练实例在每时刻的梯度相加。

- 1、**要求：每个时间的梯度都计算出来t=0,t=1,t=2,t=3,t=4，然后加起来的梯度, 为每次W更新的梯度值。**
- 2、求不同参数的导数步骤：
    - 最后一个cell:
        - 计算最后一个时刻交叉熵损失对于s_t的梯度，记忆交叉熵损失对于$s^t$ ,V,by的导数
        - 按照图中顺序计算
    - 最后一个前面的cell:
        - 第一步：求出当前层损失对于当前隐层状态输出值 $s^{t}$  的梯度 + 上一层相对于 $s^{t}$ 的损失
        - 第二步：计算tanh激活函数的导数
        - 第三步：计算 $Ux_t + Ws_{t-1} + b_{a}$ 的对于不同参数的导数

$$
\mathrm{s}^t=tanh(U\mathrm{x}^t+W\mathrm{s}^{t-1}+b_a)\\
o^{t} = softmax(V{s^t + b_y})
$$

<img src="images/RNN反向传播总结.png" alt="RNN反向传播总结" style="zoom:50%;" />

### 梯度消失与梯度爆炸

由于RNN当中也存在链式求导规则，并且其中序列的长度位置。所以

- 如果矩阵中有非常小的值，并且经过矩阵相乘N次之后，梯度值快速的以指数形式收缩，较远的时刻梯度变为0。
- 如果矩阵的值非常大，就会出现梯度爆炸

## GRU

门控循环单元



## LSTM

