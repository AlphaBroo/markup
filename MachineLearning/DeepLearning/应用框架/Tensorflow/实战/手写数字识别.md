# 手写数字识别

## 数据集介绍

[数据集地址](http://yann.lecun.com/exdb/mnist/)

Minist数据获取API

```python
# TensorFlow框架自带了获取这个数据集的接口，所以不需要自行读取
# 旧版本tensorflow
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
mnist.train.next_batch(100)	# 批量获取数据功能
mnist.train.images  # 训练数据图片
mnist.train.labels	# 训练数据图片对应的目标值(经过one-hot处理)
mnist.test.images	# 测试数据图片
mnist.test.labels	# 测试数据图片对应的目标值(经过one-hot处理)
# 新版本
import tensorflow as tf
data = tf.keras.datasets.mnist
(train_x, train_y), (test_x, test_y) = data.load_data()
train_label = tf.keras.utils.to_categorical(train_label, num_classes=10)  # one-hot处理
test_label = tf.keras.utils.to_categorical(test_label, num_classes=10)  # one-hot处理
```

数据集内容

```
数据集分为两部分：55000行训练数据集(mnist.tain);10000行测试数据集(mnist.test)
每个MINIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签
图片为黑白图片，每张图片包含28像素*28像素
每个图片都有相应的标签，0～9之间的数字表示图像中绘制的内容。使用了one-hot编码
```

## 单层全连接

```python
import os

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 1.利用数据，在训练的时候实时提供数据
# minist手写数字数据在运行时候实时提供给占位符

tf.app.flags.DEFINE_integer("is_train", 1, "指定是否是训练模型，还是拿数据去预测")
FLAGS = tf.app.flags.FLAGS


def full_connection():
    """
    用单层全连接来对手写数字进行识别
    特征值：[None, 784]
    目标值： one-hot [None, 10]
    """
    # 1.准备数据
    minist = input_data.read_data_sets("../minist_data", one_hot=True)
    with tf.variable_scope("minist_data"):
        x = tf.placeholder(dtype=tf.float32, shape=[None, 784])
        y_t = tf.placeholder(dtype=tf.float32, shape=[None, 10])
    # 2.构建模型
    # 类别：10个类别，全连接层：10个神经元
    # 全连接层神经网络计算公式：[None, 784] * [784,10] = [None,10]
    # 随机初始化权重偏置参数，这些是优化的参数，必须使用变量op去定义
    with tf.variable_scope("fc_model"):
        weights = tf.Variable(initial_value=tf.random_normal(shape=[784, 10]), name="w")
        bias = tf.Variable(initial_value=tf.random_normal(shape=[10]), name="b")
        # fc层的计算，[None, 10]输出结果，提供给softmax使用
        y_p = tf.matmul(x, weights) + bias
    # 3.构造损失函数
    # softmax回归及交叉熵损失计算
    with tf.variable_scope("softmax_cross_entropy"):
        # labels：真实值，[None, 10]
        # logits: 全连接层的输出 [None, 10]
        # 返回每个样本的损失组成的列表
        error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_t, logits=y_p))
    # 4.优化损失
    # 梯度下降优化损失
    with tf.variable_scope("optimizer"):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(error)
    # 5.准确率计算
    with tf.variable_scope("accuracy"):
        # 1)比较输出的结果最大值所在位置和真实值的最大值所在位置
        equal_list = tf.equal(tf.argmax(y_t, 1), tf.argmax(y_p, 1))
        # 2)求平均
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    # （2）收集要显示的变量
    tf.summary.scalar("error", error)  # 损失
    tf.summary.scalar("accuracy", accuracy)  # 准确率
    tf.summary.histogram("weights", weights)  # 权重
    tf.summary.histogram("bias", bias)  # 偏置
    # 初始化变量
    init = tf.global_variables_initializer()
    # （3）合并所有变量op
    merged = tf.summary.merge_all()

    # 创建模型保存和加载
    saver = tf.train.Saver()

    # 开启会话
    with tf.Session() as sess:
        # 初始化变量
        sess.run(init)
        # tensorboard可视化
        # （1）创建一个events文件实例
        file_writer = tf.summary.FileWriter("./tmp/summary/", graph=sess.graph)
        # 加载模型
        if os.path.exists("./tmp/modelckpt/checkpoint"):
            saver.restore(sess, "./tmp/modelckpt/fc_nn_model")

        if FLAGS.is_train == 1:
            # 循环步数去训练
            for i in range(1000):
                # 获取数据，实时提供
                # 每步提供50个样本训练
                image, label = minist.train.next_batch(50)
                print("训练之前，损失为:{}".format(sess.run(error, feed_dict={x: image, y_t: label})))
                #  开始训练
                _, loss, accuracy_v = sess.run([optimizer, error, accuracy], feed_dict={x: image, y_t: label})
                print("第{}次训练，损失为:{}, 准确率:{}".format(i + 1, loss, accuracy_v))

                # （4）运行合并，写入事件文件当中
                summary = sess.run(merged, feed_dict={x: image, y_t: label})
                file_writer.add_summary(summary, i)
                # 保存模型文件
                if i % 100 == 0:
                    saver.save(sess, "./tmp/modeelckpt/fc_nn_model")

        else:
            # 若不是训练，则是预测测试集数据
            for i in range(100):
                # 每次拿一个样本
                image, label = minist.test.next_batch(1)
                print("第{}个样本的真实值是：{}, 模型的预测结果是：{}".format(
                    i + 1,
                    tf.argmax(sess.run(y_t, feed_dict={x: image, y_t: label}), 1).eval(),
                    tf.argmax(sess.run(y_p, feed_dict={x: image, y_t: label}), 1).eval()
                ))


def main(argv):
    full_connection()


if __name__ == "__main__":
    tf.app.run()

```

## 卷积神经网络

```python
import os

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 1.利用数据，在训练的时候实时提供数据
# minist手写数字数据在运行时候实时提供给占位符

tf.app.flags.DEFINE_integer("is_train", 1, "指定是否是训练模型，还是拿数据去预测")
FLAGS = tf.app.flags.FLAGS


def create_weights(shape):
    return tf.Variable(initial_value=tf.random_normal(shape=shape))


def create_model(x):
    """
    构建卷积神经网络
    :return:
    """
    y_p = 0
    # 1.第一个卷积大层
    with tf.variable_scope("conv1"):
        # 卷积层
        # 将x[None, 784]形状进行修改
        input_x = tf.reshape(x, shape=[-1, 28, 28, 1])
        # 定义filter和偏置
        conv1_weights = create_weights(shape=[5, 5, 1, 32])
        conv1_bias = create_weights(shape=[32])
        conv1_x = tf.nn.conv2d(input=input_x, filter=conv1_weights, strides=[1, 1, 1, 1], padding="SAME") + conv1_bias
        # 激活层
        relu1_x = tf.nn.relu(conv1_x)
        # 池化层
        pool1_x = tf.nn.max_pool(value=relu1_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
    # 2.第二个卷积大层
    with tf.variable_scope("conv2"):
        # 卷积层
        # 定义filter和偏置
        conv2_weights = create_weights(shape=[5, 5, 32, 64])
        conv2_bias = create_weights(shape=[64])
        conv2_x = tf.nn.conv2d(pool1_x, filter=conv2_weights, strides=[1, 1, 1, 1], padding="SAME") + conv2_bias
        # 激活层
        relu2_x = tf.nn.relu(conv2_x)
        # 池化层
        pool2_x = tf.nn.max_pool(value=relu2_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
    # 3.全连接
    with tf.variable_scope("full_connection"):
        x_fc = tf.reshape(pool2_x, shape=[-1, 7 * 7 * 64])
        weights_fc = create_weights(shape=[7 * 7 * 64, 10])
        bias_fc = create_weights(shape=[10])
        y_p = tf.matmul(x_fc, weights_fc) + bias_fc
    return y_p


def full_connection():
    """
    用单层全连接来对手写数字进行识别
    特征值：[None, 784]
    目标值： one-hot [None, 10]
    """
    # 1.准备数据
    minist = input_data.read_data_sets("../minist_data", one_hot=True)
    with tf.variable_scope("minist_data"):
        x = tf.placeholder(dtype=tf.float32, shape=[None, 784])
        y_t = tf.placeholder(dtype=tf.float32, shape=[None, 10])
    # 2.构建模型
    # 卷积神经网络
    y_p = create_model(x)
    # 3.构造损失函数
    # softmax回归及交叉熵损失计算
    with tf.variable_scope("softmax_cross_entropy"):
        # labels：真实值，[None, 10]
        # logits: 全连接层的输出 [None, 10]
        # 返回每个样本的损失组成的列表
        error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_t, logits=y_p))
    # 4.优化损失
    # 梯度下降优化损失
    with tf.variable_scope("optimizer"):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(error)
    # 5.准确率计算
    with tf.variable_scope("accuracy"):
        # 1)比较输出的结果最大值所在位置和真实值的最大值所在位置
        equal_list = tf.equal(tf.argmax(y_t, 1), tf.argmax(y_p, 1))
        # 2)求平均
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    # （2）收集要显示的变量
    tf.summary.scalar("error", error)  # 损失
    tf.summary.scalar("accuracy", accuracy)  # 准确率
    # 初始化变量
    init = tf.global_variables_initializer()
    # （3）合并所有变量op
    merged = tf.summary.merge_all()

    # 创建模型保存和加载
    saver = tf.train.Saver()

    # 开启会话
    with tf.Session() as sess:
        # 初始化变量
        sess.run(init)
        # tensorboard可视化
        # （1）创建一个events文件实例
        file_writer = tf.summary.FileWriter("./tmp/summary/", graph=sess.graph)
        # 加载模型
        if os.path.exists("./tmp/modelckpt/checkpoint"):
            saver.restore(sess, "./tmp/modelckpt/c_nn_model")

        if FLAGS.is_train == 1:
            # 循环步数去训练
            for i in range(1000):
                # 获取数据，实时提供
                # 每步提供50个样本训练
                image, label = minist.train.next_batch(50)
                print("训练之前，损失为:{}".format(sess.run(error, feed_dict={x: image, y_t: label})))
                #  开始训练
                _, loss, accuracy_v = sess.run([optimizer, error, accuracy], feed_dict={x: image, y_t: label})
                print("第{}次训练，损失为:{}, 准确率:{}".format(i + 1, loss, accuracy_v))

                # （4）运行合并，写入事件文件当中
                summary = sess.run(merged, feed_dict={x: image, y_t: label})
                file_writer.add_summary(summary, i)
                # 保存模型文件
                if i % 100 == 0:
                    saver.save(sess, "./tmp/modeelckpt/c_nn_model")

        else:
            # 若不是训练，则是预测测试集数据
            for i in range(100):
                # 每次拿一个样本
                image, label = minist.test.next_batch(1)
                y_t_t = tf.argmax(sess.run(y_t, feed_dict={x: image, y_t: label}), 1)
                y_t_p = tf.argmax(sess.run(y_p, feed_dict={x: image, y_t: label}), 1)
                y_t_a = tf.reduce_mean(tf.cast(tf.equal(y_t_t, y_t_p), tf.float32))
                print("第{}个样本的真实值是：{}, 模型的预测结果是：{}, 准确率为：{}".format(
                    i + 1,
                    y_t_t.eval(),
                    y_t_p.eval(),
                    y_t_a.eval()
                ))


def main(argv):
    full_connection()


if __name__ == "__main__":
    tf.app.run()

```

