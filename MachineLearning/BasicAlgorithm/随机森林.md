# 随机森林

## 原理

Bagging + Base Estimator: Decision Tree

决策树在节点划分上，在随机的特征子集上寻找最优划分特征

- Extra-Tree

决策树在节点划分上，使用随机的特征和随机的阈值

提供了额外的随机性，抑制过拟合，但增大了bias

更快的训练速度

- 优点

```
1. 能够处理很高维度(feature很多)的数据，并且不用做特征选择
2. 在训练完成后，能够给出哪些feature比较重要
3. 容易做成并行化方法，速度比较快
4. 可以进行可视化， 便于分析
```

注意：理论上越多的树效果越好，但实际上基本超过一定数量就上下浮动了

## sklearn

### 随机森林

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=666)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

rf_clf = RandomForestClassifier(
		n_estimators=500,
  	random_state=666,
  	oob_score=True,
  	n_jobs=-1
)
rf_clf.fit(X, y)
print(rf_clf.oob_score_)

# 修改参数
rf_clf2 = RandomForestClassifier(
		n_estimators=500,
  	max_leaf_nodes=16,
  	random_state=666,
  	oob_score=True,
  	n_jobs=-1
)
rf_clf2.fit(X, y)
print(rf_clf2.oob_score_)
```

### Extra-Tree

```python
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(
		n_estimators=500,
  	bootstrap=True,
  	oob_score=True,
  	random_state=666
)
et_clf.fit(X, y)
print(et_clf.oob_score_)
```

### 解决回归问题

```python
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreeRegressor
```

