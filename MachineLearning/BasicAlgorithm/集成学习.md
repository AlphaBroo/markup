# 集成学习

让机器学习效果更好，多个训练算法求集成

## 原理

- 如何集成结果

hard voting：无权重，少数服从多数

Soft voting：有权重，更合理的投票

- 模型差异化

每个子模型只看样本数据的一部分

每个子模型不需要太高的准确率

- 采样

针对样本进行随机采样

```
不放回取样(Pasting)
放回取样(Bagging)，Bagging更常用，统计学中，也称为bootstrap

- out of Bag
放回取样导致一部分样本有可能没有取到
平均大约有37%的样本没有取到
不使用测试数据集，而使用这部分没有取到的样本做测试、验证
```

针对特征进行随机采样(Random Subspaces)

既针对样本又针对特征进行随机采样(Random Patches)

- Bagging

训练多个分类器并行取平均，常用算法：随机森林
$$
f(x) = \frac{1}{m}\sum_{m=1}^Mf_m(x)
$$

- Boosting

集成多个模型，每个模型都在尝试增强(Boosting)整体的效果
$$
F_m(x) = F_{m-1}(x) + argmin_h\sum_{i=1}^n{L(y_i, F_{m-1}(x_i)+h(x_i))}
$$
AdaBoosting

```
根据前一次的分类效果调整数据权重
如何某一个数据在这次分错了，那么在下一次就给它更大的权重
最终：每个分类器根据自身的准确型来确定各自的权重，再合体
```

Gradient Boosting

Xgboost

- Stacking

聚合多个分类或回归模型，可分阶段获得结果

第一阶段：采用多个模型进行预测数据得出各自结果

第二阶段：用第一阶段的结果训练得到最终结果

注意：确实可以提升结果，但是速度是限制

New instance —> Predict —> Predictions —> Blending

## 实现

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# 逻辑回归
log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
print(log_clf.score(X_test, y_test))

# svm
svm_clf = SVC()
svm_clf.fit(X_train, y_train)
print(svm_clf.score(X_test, y_test))

# 决策树
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
print(dt_clf.score(X_test, y_test))


y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)
# 手动投票
y_predict = np.array((y_redict1 + y_predict2 + y_predict3) >= 2, dtype='int')
print(accuracy_score(y_test, y_predict))

```

## sklearn

### Hard Voting

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

voting_clf = VotingClassifier(estimators=[
  	('log_clf', LogisticRegression()),
  	('svm_clf', SVC()),
  	('dt_clf', DecisionTreeClassifier(random_state=666)),
], voting='hard')

voting_clf.fit(X_train, y_train)
print(voting_clf.score(X_test, y_test))
```

### Soft Voting

要求集合的每一个模型都能估计概率

逻辑回归：本身就是基于概率模型的

KNN：

决策树：

SVM：probability参数

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

voting_clf2 = VotingClassifier(estimators=[
  	('log_clf', LogisticRegression()),
  	('svm_clf', SVC(probability=True)),
  	('dt_clf', DecisionTreeClassifier(random_state=666)),
], voting='soft')

voting_clf2.fit(X_train, y_train)
print(voting_clf2.score(X_test, y_test))
```

### Bagging/Pasting

同一种算法，不同的取样

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# BaggingClassifier
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  # 子模型个数
  	max_samples=100,  # 每个子模型中样本数
  	bootstrap=True  # 有放回取样
)

bagging_clf.fit(X_train, y_train)
print(bagging_clf.score(X_test, y_test))

bagging_clf2 = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=5000,
  	max_samples=100,
  	bootstrap=True
)

bagging_clf2.fit(X_train, y_train)
print(bagging_clf2.score(X_test, y_test))
```

### oob/n_jobs

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# BaggingClassifier
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  # 子模型个数
  	max_samples=100,  # 每个子模型中样本数
  	bootstrap=True,  # 有放回取样
  	oob_score=True,  # 不分测试集
)

bagging_clf.fit(X, y)
print(bagging_clf.oob_score_)

# 并行n_jobs
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=100, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs = -1  # 并行
)
bagging_clf.fit(X, y)
```

### bootstrap_features

```python
# 对样本特征随机采样
random_subspaces_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=500, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs=-1,
  	max_features=1,  # 对特征随机取样,由于仅有2个特征
  	bootstrap_features=True  # 对特征采样放回取样 
)
random_subspaces_clf.fit(X, y)
print(random_subspaces_clf.oob_score_)

# 对样本数和样本特征都随机采样
random_patches_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=500, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs=-1,
  	max_features=1,  # 对特征随机取样,由于仅有2个特征
  	bootstrap_features=True  # 对特征采样放回取样 
)
random_patches_clf.fit(X, y)
print(random_patches_clf.oob_score_)
```

### Boosting

不再有oob_score数据集

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# AdaBoosting
ada_clf = AdaBoostClassifier(
		DecisionTreeClassifier(max_depth=2),
  	n_estimators=500
)
ada_clf.fit(X_train, y_train)
print(ad_clf.score(X_test, y_test)
      
# Gradient Boosting
gb_clf = GradientBoostingClassifier(
		max_depth=2,
  	n_estimators=30
)
gb_clf.fit(X_train, y_train)
print(gb_clf.score(X_test, y_test)
```

### Boosting解决回归问题

```python
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
```

