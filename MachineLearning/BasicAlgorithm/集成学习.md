# 集成学习

## 原理

- hard voting

无权重，少数服从多数

- Soft voting

更合理的投票，

- 差异化

每个子模型只看样本数据的一部分

每个子模型不需要太高的准确率

取样：放回取样(Bagging)，不放回取样(Pasting)

Bagging更常用，统计学中，也称为bootstrap

- Out-of-Bag

放回取样导致一部分样本有可能没有取到

平均大约有37%的样本没有取到

不使用测试数据集，而使用这部分没有取到的样本做测试、验证

- 其他采样

针对特征进行随机采样(Random Subspaces)

既针对样本又针对特征进行随机采样(Random Patches)

- Boosting

集成多个模型，每个模型都在尝试增强(Boosting)整体的效果

Ada Boosting

Gradient Boosting

- Stacking

New instance —> Predict —> Predictions —> Blending

## 实现

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from 

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# 逻辑回归
log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
print(log_clf.score(X_test, y_test))

# svm
svm_clf = SVC()
svm_clf.fit(X_train, y_train)
print(svm_clf.score(X_test, y_test))

# 决策树
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
print(dt_clf.score(X_test, y_test))


y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)
# 手动投票
y_predict = np.array((y_redict1 + y_predict2 + y_predict3) >= 2, dtype='int')
print(accuracy_score(y_test, y_predict))

```

## sklearn

### Hard Voting

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

voting_clf = VotingClassifier(estimators=[
  	('log_clf', LogisticRegression()),
  	('svm_clf', SVC()),
  	('dt_clf', DecisionTreeClassifier(random_state=666)),
], voting='hard')

voting_clf.fit(X_train, y_train)
print(voting_clf.score(X_test, y_test))
```

### Soft Voting

要求集合的每一个模型都能估计概率

逻辑回归：本身就是基于概率模型的

KNN：

决策树：

SVM：probability参数

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

voting_clf2 = VotingClassifier(estimators=[
  	('log_clf', LogisticRegression()),
  	('svm_clf', SVC(probability=True)),
  	('dt_clf', DecisionTreeClassifier(random_state=666)),
], voting='soft')

voting_clf2.fit(X_train, y_train)
print(voting_clf2.score(X_test, y_test))
```

### Bagging/Pasting

同一种算法，不同的取样

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# BaggingClassifier
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  # 子模型个数
  	max_samples=100,  # 每个子模型中样本数
  	bootstrap=True  # 有放回取样
)

bagging_clf.fit(X_train, y_train)
print(bagging_clf.score(X_test, y_test))

bagging_clf2 = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=5000,
  	max_samples=100,
  	bootstrap=True
)

bagging_clf2.fit(X_train, y_train)
print(bagging_clf2.score(X_test, y_test))
```

### oob/n_jobs

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# BaggingClassifier
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  # 子模型个数
  	max_samples=100,  # 每个子模型中样本数
  	bootstrap=True,  # 有放回取样
  	oob_score=True,  # 不分测试集
)

bagging_clf.fit(X, y)
print(bagging_clf.oob_score_)

# 并行n_jobs
bagging_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=100, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs = -1  # 并行
)
bagging_clf.fit(X, y)
```

### bootstrap_features

```python
# 对样本特征随机采样
random_subspaces_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=500, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs=-1,
  	max_features=1,  # 对特征随机取样,由于仅有2个特征
  	bootstrap_features=True  # 对特征采样放回取样 
)
random_subspaces_clf.fit(X, y)
print(random_subspaces_clf.oob_score_)

# 对样本数和样本特征都随机采样
random_patches_clf = BaggingClassifier(
  	DecisionTreeClassifier(),
		n_estimators=500,  
  	max_samples=500, 
  	bootstrap=True,
  	oob_score=True,
  	n_jobs=-1,
  	max_features=1,  # 对特征随机取样,由于仅有2个特征
  	bootstrap_features=True  # 对特征采样放回取样 
)
random_patches_clf.fit(X, y)
print(random_patches_clf.oob_score_)
```

### Boosting

不再有oob_score数据集

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=42)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# AdaBoosting
ada_clf = AdaBoostClassifier(
		DecisionTreeClassifier(max_depth=2),
  	n_estimators=500
)
ada_clf.fit(X_train, y_train)
print(ad_clf.score(X_test, y_test)
      
# Gradient Boosting
gb_clf = GradientBoostingClassifier(
		max_depth=2,
  	n_estimators=30
)
gb_clf.fit(X_train, y_train)
print(gb_clf.score(X_test, y_test)
```

### Boosting结局回归问题

```python
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
```

