# 决策树

非参数学些算法

可以解决分类问题

天然可以解决多分类问题

也可以解决回归问题

非常好的可解释性

- 局限性

```
决策边界横平竖直，对于倾斜的样本分类易过拟合

对个别数据敏感
```

## 原理

划分标准

### 信息熵

熵在信息论中代表随机变量不确定度的度量

熵越大，数据的不确定性越高；熵越小，数据的不确定性越低
$$
H = -\sum_{i=1}^k{p_ilog(p_i)}
$$
二分类
$$
H = -xlog(x)-(1-x)log(1-x)
$$

### 基尼系数

基尼系数越高，数据的不确定性越高，基尼系数越小，数据的不确定性越低
$$
G = \sum_{i=1}^k{p_i}(1-p_i) = 1 - \sum_{i=1}^k{p_i^2}
$$
二分类
$$
G = 1-x^2 - (1-x)^2 = -2x^2 + 2x
$$

- 信息熵VS基尼系数

信息熵的计算比基尼系数稍慢

大多数时候二者没有特别的效果优劣

### CART

Classification And Regression Tree

根据某一个维度d和某一个阈值v进行二分

sklearn的决策树实现是CART，其他的实现方式有ID3, C4.5, C5.0

```
ID3		信息增益
C4.5	信息增益率(解决ID3问题，考虑自身熵)
CART	使用GINI系数当作衡量标准
```

预测时间复杂度:$O(logm)$

训练时间复杂度:$O(n*m*logm)$

由于常常产生过拟合，故采用剪枝

- 剪枝

剪枝：降低复杂度，解决过拟合

剪枝：预剪枝(更常用)、后剪枝

预剪枝：限制深度，叶子节点个数，叶子节点样本数，信息增益量等

后剪枝：通过一定的衡量标准$C_{\alpha}(T)=C(T)+\alpha\cdot|T_{leaf}|$，叶子节点越多，损失越大（$C(T)$是叶子节点gini*叶子节点样本数，$T_{leaf}$是叶子节点数）

## 实现

### 信息熵

```python
import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
      return -p * np.log(p) - (1 - p) * np.log(1 - p)

x = np.linspace(0.01, 0.99, 200)
plt.plot(x, entropy(x))
plt.show()
# 在两类别信息平均分配时，系统最不稳定
```

### 寻找最优划分

使用信息熵

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from collections import Counter
from math import log

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个维度特征
y = iris.target


dt_clf = DecisionTreeClassifier(max_depth=2, criterion="gini")
dt_clf.fit(X, y)

def split(X, y, d, value):
      index_a = (X[:, d] <= value)
    index_b = (X[:, d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

def entropy(y):
      counter = Counter(y)
    res = 0.0
    for num in counter.value():
          p = num / len(y)
        res += -p * log(p)
    return res

def try_split(X, y):
      best_entropy = float('inf')  # 正无穷
    best_d, best_v = -1, -1
    for d in range(X.shpe[1]):
          sorted_index = np.argsort(X[:, d])
        for i in range(1, len(X)):
              if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                      v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                X_l, X_r, y_l, y_r = split(X, y, d, v)
                    e = entropy(y_l) + entropy(y_r)
                if e < best_entropy:
                      best_entropy, best_d, best_v = e, d, v
    return best_entropy, best_d, best_v

best_entropy, best_d, best_v = try_split(X, y)
print("best_entropy =", best_entropy)
print("best_d =", best_d)
print("best_v =", best_v)

X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
print(entropy(y1_1))
print(entropy(y1_r))

best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)
print("best_entropy =", best_entropy2)
print("best_d =", best_d2)
print("best_v =", best_v2)

X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
print(entropy(y2_1))
print(entropy(y2_r))
```

使用基尼系数

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from collections import Counter
from math import log

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个维度特征
y = iris.target

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()

dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

def split(X, y, d, value):
      index_a = (X[:, d] <= value)
    index_b = (X[:, d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

def gini(y):
      counter = Counter(y)
    res = 1.0
    for num in counter.value():
          p = num / len(y)
        res -= p ** 2
    return res

def try_split(X, y):
      best_g = float('inf')
    best_d, best_v = -1, -1
    for d in range(X.shpe[1]):
          sorted_index = np.argsort(X[:, d])
        for i in range(1, len(X)):
              if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                      v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                X_l, X_r, y_l, y_r = split(X, y, d, v)
                    g = gini(y_l) + gini(y_r)
                if e < best_g:
                      best_g, best_d, best_v = g, d, v
    return best_g, best_d, best_v

best_g, best_d, best_v = try_split(X, y)
print("best_g =", best_g)
print("best_d =", best_d)
print("best_v =", best_v)

X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
print(gini(y1_1))
print(gini(y1_r))

best_g2, best_d2, best_v2 = try_split(X1_r, y1_r)
print("best_g =", best_g2)
print("best_d =", best_d2)
print("best_v =", best_v2)

X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
print(gini(y2_1))
print(gini(y2_r))
```

## sklearn

### 分类

#### 不同方法划分

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个特征
y = iris.target

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()

# 决策边界
def plot_decision_boundary(model, axis):
      x0, x1 = np.meshgrid(
            np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1)
          np.linspace(axis[2], axisp3), int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D", "#90CAF9"])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)

# 使用信息熵
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 

# 使用基尼系数
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="gini")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 
```

#### 超参数剪枝

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

X, y = datasets.make_noons(noise=0.25, random_state=666)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 决策边界
def plot_decision_boundary(model, axis):
      x0, x1 = np.meshgrid(
            np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1)
          np.linspace(axis[2], axisp3), int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D", "#90CAF9"])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)


# 默认使用基尼系数划分, 无深度限制，过拟合
dt_clf = DecisionTreeClassifier()  
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 深度降低为2，泛化能力增强
dt_clf2 = DecisionTreeClassifier(max_depth=2)  
dt_clf2.fit(X, y)

plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 最小可划分叶子节点样本数，若节点样本数少于此，则不会再继续选择最优特征进行划分
# min_samples_split值越高，泛化能力越强
dt_clf3 = DecisionTreeClassifier(min_samples_split=10)  
dt_clf3.fit(X, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 叶子节点最少样本数，若样本数少于此，则被剪枝
# min_samples_leaf值越高，泛化能力越强
dt_clf4 = DecisionTreeClassifier(min_samples_leaf=10)  
dt_clf4.fit(X, y)

plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 最大叶子节点数，可以防止过拟合
# max_leaf_nodes值越高，泛化能力越差
dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=4)  
dt_clf5.fit(X, y)

plot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
```

#### 并查集的局限性

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个特征
y = iris.target

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()

# 决策边界
def plot_decision_boundary(model, axis):
      x0, x1 = np.meshgrid(
            np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1)
          np.linspace(axis[2], axisp3), int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D", "#90CAF9"])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)

# 使用信息熵
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 

# 删除特殊点
X_new = np.delete(X, 138, axis=0)
y_new = np.delete(y, 138)

tree_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
tree_clf.fit(X_new, y_new)

plot_decision_boundary(tree_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 
```

### 回归

决策树也可以解决回归问题

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor


boston = datasets.load_boston()
X = boston.data
y = boston.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)
print(dt_reg.score(X_test, y_test))
print(dt_reg.score(X_train, y_train))  # 过拟合

# 可以通过调参来改善过拟合现象
```

## 可视化

安装

```
1. graphviz(官网下载)
2. pip install pydotplus
```

使用

```python
import pydotplus
from IPython.display import Image

# 通过graphviz生成dot数据
dot_data = tree.export_graphviz(
	dt_reg,  # 训练后的模型
  out_file = None,
  feature_names = housing.feature_names[6:8],  # 特征
  filled = True,
  impurity = False,
  rounded = True
)

graph = pydotplus.graph_from_dot_data(dot_data)
graph.get_nodes()[7].set_fillcolor("#FFF2DD")
Image(graph.create_png())
graph.write_png("demo.png")
```

