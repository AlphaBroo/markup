# 贝叶斯

优缺点

```
# 优点
1.朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
2.对缺失数据不太敏感，算法也比较简单，常用于文本分类。
3.分类准确度高，速度快

# 缺点
1.需要知道先验概率P(F1,F2,…|C)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳
```

## 原理

$$
p(B|A)=\frac{p(A|B)p(B)}{p(A)}
$$

当事件(特征)相互独立时，贝叶斯准则转变为朴素贝叶斯，朴素贝叶斯是贝叶斯准则中的一种特殊情况。

```python
# 朴素贝叶斯分类
sklearn.naive_bayes.MultinomialNB(alpha = 1.0)

# 输入
alpha：拉普拉斯平滑系数
```

- 拉普拉斯平滑

```python
由于样本数较少，会出现p(A|B)的概率为0，防止此情况出现，使用拉普莱斯平滑

拉普拉斯平滑系数ɑ, 默认为1
p=Ni/N    ---> p=(Ni+a)/(N+am)
m为训练文档中特征词个数，Ni为xi在分类ci下出现的次数，N为分类ci下词频总数。
```

## sklearn

### API

```python
# 朴素贝叶斯分类
sklearn.naive_bayes.MultinomialNB(alpha = 1.0)

# 输入
alpha：拉普拉斯平滑系数
```

### 示例

```python
# 1.导入需要的包
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

# 2.载入数据
news = fetch_20newsgroups(subset="all")
# 3.特征选取
# 特征值,文章内容
x = news.data
# 目标值，文章的类别
y = news.target
print(len(y))
# 4.分割训练集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)
# 5.TF-IDF生成文章特征词
# 特征抽取
cv = TfidfVectorizer()
x_train = cv.fit_transform(x_train)  # 词频矩阵
x_test = cv.transform(x_test)  # 按照训练集抽取特征词统计词频
# 6.朴素贝叶斯estimator流程进行预估
mnb = MultinomialNB()
mnb.fit(x_train, y_train)
mnb.predict(x_test)
score = mnb.score(x_test, y_test)
print(score)
```

