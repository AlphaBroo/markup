# 随机森林

随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

是一种集成方法，通过集成多个比较简单的评估器形成累积效果。

优缺点

```
# 优点
1.能够解决单个决策树不稳定的情况
2.能够处理具有高维特征的输入样本，而且不需要降维（使用的是特征子集）
3.对于缺省值问题也能够获得很好得结果（投票）

# 缺点
随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合
```

## 原理

Bagging + Base Estimator: Decision Tree

决策树在节点划分上，在随机的特征子集上寻找最优划分特征

- Extra-Tree

决策树在节点划分上，使用随机的特征和随机的阈值

提供了额外的随机性，抑制过拟合，但增大了bias

更快的训练速度

- 优点

```
1. 能够处理很高维度(feature很多)的数据，并且不用做特征选择
2. 在训练完成后，能够给出哪些feature比较重要
3. 容易做成并行化方法，速度比较快
4. 可以进行可视化， 便于分析
```

注意：理论上越多的树效果越好，但实际上基本超过一定数量就上下浮动了

## sklearn

### API

```python
# 1.袋装算法
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

tree = DecisionTreeClassifier()
bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)
bag.fit(X, y)


# 随机森林分类器
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None,bootstrap=True, random_state=None)
# 参数
# n_estimators：integer，optional（default = 10） 森林里的树木数量
# criteria：string，可选（default =“gini”）分割特征的测量方法
# max_depth：integer或None，可选（默认=无）树的最大深度 
# bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样 
```

### 随机森林

```python
import numpy as np
import matplotlib.pylot as plt
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier

X, y = datasets.make_noons(n_samples=500, noise=0.3, random_state=666)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

rf_clf = RandomForestClassifier(
		n_estimators=500,
  	random_state=666,
  	oob_score=True,
  	n_jobs=-1
)
rf_clf.fit(X, y)
print(rf_clf.oob_score_)

# 修改参数
rf_clf2 = RandomForestClassifier(
		n_estimators=500,
  	max_leaf_nodes=16,
  	random_state=666,
  	oob_score=True,
  	n_jobs=-1
)
rf_clf2.fit(X, y)
print(rf_clf2.oob_score_)
```

### Extra-Tree

```python
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(
		n_estimators=500,
  	bootstrap=True,
  	oob_score=True,
  	random_state=666
)
et_clf.fit(X, y)
print(et_clf.oob_score_)
```

### 解决回归问题

```python
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreeRegressor
```

示例

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestRegressor


# 快慢震荡组合数据
def model(x, sigma=0.3):
    fast_oscillation = np.sin(5 * x)
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * rng.randn(len(x))
    return slow_oscillation + fast_oscillation + noise


rng = np.random.RandomState(42)
x = 10 * rng.rand(200)

y = model(x)
# plt.errorbar(x, y, 0.3, fmt='o')
# plt.show()

# 使用随机森林回归
forest = RandomForestRegressor(200)
forest.fit(x[:, None], y)

xfit = np.linspace(0, 10, 1000)
yfit = forest.predict(xfit[:, None])
ytrue = model(xfit, sigma=0)

plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit, '-g')
plt.plot(xfit, ytrue, '-k', alpha=0.5)
plt.show()

```

### 示例

示例1

```python
import numpy as np

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier


# 对分类器结果进行可视化
def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):
    ax = ax or plt.gca()
    # 画出训练数据
    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    # 用评估器拟合数据
    model.fit(X, y)
    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    # 为结果生成彩色图
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5,
                           cmap=cmap, zorder=1)
    ax.set(xlim=xlim, ylim=ylim)


# 创建一颗决策树
X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plt.show()


# 集成袋装分类器
# tree = DecisionTreeClassifier()
# bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)
# visualize_classifier(bag, X, y)
# plt.show()

# 随机森林
model = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_classifier(model, X, y)
plt.show()
```

示例2

```python
# 1.导入合适的包
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,export_graphviz
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier

# 2.加载数据
data = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")
# 3.数据处理
# 填补缺失值age
data["age"].fillna(data["age"].mean(), inplace=True)
# 雷彪数据进行One-Hot编码
data = pd.get_dummies(data, columns=["pclass", "sex"]) 
print(data.head(2))
# 4.特征选择和数据集分割
# 特征值
x = data[["age", "pclass_1st", "pclass_2nd", "pclass_3rd", "sex_female", "sex_male"]]
# 目标值
y = data["survived"]
# 数据集分割
x_train, x_test, y_train, y_test = train_test_split(x, y)
# 5.随机森林估计器流程
rfc = RandomForestClassifier(n_estimators=5, criterion="entropy", max_depth=4)
rfc.fit(x_train, y_train)
# 5.预测
predict = rfc.predict(x_test)
# 6.准确率
score = rfc.score(x_test, y_test)
print(score)
```

