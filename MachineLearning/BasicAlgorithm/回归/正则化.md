# 正则化

## 岭回归

Ridge Regression，$L_2$ 范数正则化

### 原理

损失函数
$$
J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\frac{1}{2}\sum_{i=1}^n{\theta_i^2}
$$
使$J(\theta)$和$\theta$尽量小，采用了模型正则化，$\alpha$为超参数

### sklearn

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge

np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()

# 多项式回归，过拟合
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
  
np.ramdom.seed(666)
X_train, X_test, y_train, y_test = train_split_test(X, y)

poly20_reg = PolynomialRegression(degree=20)
poly20_reg.fit(X, y)
y20_predict = poly20_reg.predict(X)
error = mean_squared_error(y, y20_predict)
print(error)

def plot_model(model):
  	X_plot = np.linspace(-3, 3, 100).reshpe(100, 1)
		y_plot = model.predict(X_plot)
		plt.scatter(x, y)
		plt.plot(X_plot[:, 0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
		plt.show()

plot_model(poly20_reg)

# 岭回归
def RidgeRegression(degree, alpha):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("ridge_reg", Ridge(alpha=alpha))
    ])

ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(X_train, y_train)
y1_predict = ridge1_reg.predict(X_test)
error = mean_squared_error(y_test, y1_predict)
print(error)

plot_model(ridge1_reg)
# 新的alpha
ridge2_reg = RidgeRegression(20, 1)
ridge2_reg.fit(X_train, y_train)
y2_predict = ridge2_reg.predict(X_test)
error = mean_squared_error(y_test, y2_predict)
print(error)

plot_model(ridge2_reg)

ridge2_reg = RidgeRegression(20, 100000)
ridge2_reg.fit(X_train, y_train)
y2_predict = ridge2_reg.predict(X_test)
error = mean_squared_error(y_test, y2_predict)
print(error)

plot_model(ridge2_reg)  # 极限状态，alpha极大，则theta为0，形成一条直线
```

## Lasso回归

LASSO Regression，$L_1$范数正则化

### 原理

损失函数
$$
J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\sum_{i=1}^n{|\theta_i|}
$$
使$J(\theta)$和$\theta$尽量小，采用了模型正则化，$\alpha$为超参数

LASSO趋向于使得一部分theta值变为0，可作为特征选择用。

缺点：可能将有效特征消除

### 实现

### sklearn

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso

np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()

np.ramdom.seed(666)
X_train, X_test, y_train, y_test = train_split_test(X, y)

# 多项式回归，过拟合
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])

poly20_reg = PolynomialRegression(degree=20)
poly20_reg.fit(X, y)
y20_predict = poly20_reg.predict(X)
error = mean_squared_error(y, y20_predict)
print(error)

def plot_model(model):
  	X_plot = np.linspace(-3, 3, 100).reshpe(100, 1)
		y_plot = model.predict(X_plot)
		plt.scatter(x, y)
		plt.plot(X_plot[:, 0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
		plt.show()

plot_model(poly20_reg)

# LASSO回归
def LassoRegression(degree, alpha):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("ridge_reg", Lasso(alpha=alpha))
    ])

lasso1_reg = LassoRegression(20, 0.001)
lasso1_reg.fit(X_train, y_train)
y1_predict = lasso1_reg.predict(X_test)
error = mean_squared_error(y_test, y1_predict)
print(error)

plot_model(lasso1_reg)

# 改变alpha
lasso2_reg = LassoRegression(20, 0.1)
lasso2_reg.fit(X_train, y_train)
y2_predict = lasso2_reg.predict(X_test)
error = mean_squared_error(y_test, y2_predict)
print(error)

plot_model(lasso2_reg)

lasso3_reg = LassoRegression(20, 1)
lasso3_reg.fit(X_train, y_train)
y3_predict = lasso3_reg.predict(X_test)
error = mean_squared_error(y_test, y3_predict)
print(error)

plot_model(lasso2_reg)
```

## 对比

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.base import BaseEstimator, TransformerMixin

rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = np.sin(x) + 0.1 * rng.randn(50)  # 带噪的正弦波

xfit = np.linspace(0, 10, 1000)


# 高斯基函数
class GaussianFeatures(BaseEstimator, TransformerMixin):
    """一维输入均匀分布的高斯特征"""

    def __init__(self, N, width_factor=2.0):
        self.N = N
        self.width_factor = width_factor

    @staticmethod
    def _gauss_basis(x, y, width, axis=None):
        arg = (x - y) / width
        return np.exp(-0.5 * np.sum(arg ** 2, axis))

    def fit(self, X, y=None):
        # 在数据区间中创建N个高斯分布中心
        self.centers_ = np.linspace(X.min(), X.max(), self.N)
        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])
        return self

    def transform(self, X):
        return self._gauss_basis(X[:, :, np.newaxis], self.centers_, self.width_, axis=1)


# model = make_pipeline(GaussianFeatures(30), LinearRegression())
# model.fit(x[:, np.newaxis], y)
# yfit = model.predict(xfit[:, np.newaxis])
# plt.scatter(x, y)
# plt.plot(xfit, yfit)
# plt.xlim(0, 10)
# plt.ylim(-1.5, 1.5)
# plt.show()
# 将数据投影到30维的基函数上，模型就会过于灵活，产生过拟合

# 将高斯基函数的系数画出
def basis_plot(model, title=None):
    fig, ax = plt.subplots(2, sharex=True)
    model.fit(x[:, np.newaxis], y)
    ax[0].scatter(x, y)
    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))

    if title:
        ax[0].set_title(title)
    ax[1].plot(model.steps[0][1].centers_, model.steps[1][1].coef_)
    ax[1].set(xlabel='basis location', ylabel='coefficent', xlim=(0, 10))


model = make_pipeline(GaussianFeatures(30), LinearRegression())
basis_plot(model)

model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1, max_iter=1000))
basis_plot(model, title='Ridge Regression')

model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))
basis_plot(model, title='Lasso Regression')

plt.show()

```

