# 线性回归

线性模型是最直观的机器学习模型，通过学习属性与目标属性的线性关系来解释数据的规律

思想简单，实现容易，结果具有良好的可解释性

可解释性

```
对于系数，若为正，表示特征与目标值正相关；若为负，表示特征与目标值负相关
```

只能解决线性问题

## 简单线性回归

### 损失函数

假设找到了最佳拟合的直线方程
$$
y = ax + b
$$
对于每一个样本点$x^{(i)}$，根据直线方程，预测值为
$$
\hat{y}^{(i)} = ax^{(i)} + b
$$
则损失函数为
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\sum_{i=1}^m{(y^{(i)}-ax^{(i)}-b)^2}=J(a,b)
$$
求$J(a,b)$函数的最小值，采用求导
$$
\frac{\partial{J(a,b)}}{\partial{a}}=\sum_{i=1}^m{2(y^{(i)}-ax^{(i)}-b)}(-x^{(i)})=0
$$

$$
\frac{\partial{J(a,b)}}{\partial{b}}=\sum_{i=1}^m{2(y^{(i)}-ax^{(i)}-b)(-1)}=0
$$

求得a,b为
$$
a = \frac{\sum_{i=1}^m{(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{\sum_{i=1}^m{(x^{(i)}-\bar{x})^2}}
$$

$$
b = \bar{y}-a\bar{x}
$$

### 最小二乘法

### 向量化

有向量
$$
w = (w^{(1)}, w^{(2)}, ...,w^{(m)})
$$

$$
v = (v^{(1)}, v^{(2)},...,v^{(m)})
$$

可将如下求和公式
$$
\sum_{i=1}^m{w^{(i)}\cdot v^{(i)}}
$$
向量化
$$
w\cdot v
$$
向量化可以提高运算速度

### 实现

- 循环

简单实现

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])

x_mean = np.mean(x)
y_mean = np.mean(y)
num = 0.0  # 分子
d = 0.0  # 分母
for x_i, y_i in zip(x, y):
  	num += (x_i-x_mean)*(y_i-y_mean)
    d += (x_i-x_mean)**2
a = num / d
b = y_mean - a * x_mean
y_hat = a * x + b

x_predict = 6
y_predict = a * x_predict + b
print(y_predict, a, b)
```

类创建

```python
# SimpleLinearRegression.py
import numpy as np
from .metrics import r2_score


class SimpleLinearRegression1:

    def __init__(self):
        """初始化Simple Linear Regression模型"""
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        """根据训练数据集x_train, y_train训练Simple Linear Regression模型"""
        assert x_train.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert len(x_train) == len(y_train), \
            "the size of x_train must be equal to the size of y_train"

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
				num = 0.0
        d = 0.0
        for x, y in zip(x_train, y_train):
            num += (x - x_mean) * (y - y_mean)
          	d += (x - x_mean) ** 2
        self.a_ = num / d
        self.b_ = y_mean - self.a_ * x_mean

        return self

    def predict(self, x_predict):
        """给定待预测数据集x_predict，返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, \
            "must fit before predict!"

        return np.array([self._predict(x) for x in x_predict])

    def _predict(self, x_single):
        """给定单个待预测数据x，返回x的预测结果值"""
        return self.a_ * x_single + self.b_

    def score(self, x_test, y_test):
        """根据测试数据集 x_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(x_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "SimpleLinearRegression()"
```

使用

```python
from playML.SimpleLinearRegression import SimpleLinearRegression1

reg1 = SimpleLinearRegression1()
reg1.fit(x, y)
y_predict = reg1.predict(np.array([x_predict]))
print(y_predict)
print(reg1.a_)
print(reg1.b_)
```

- 向量化

```python
# SimpleLinearRegression.py
import numpy as np
from .metrics import r2_score


class SimpleLinearRegression:

    def __init__(self):
        """初始化Simple Linear Regression模型"""
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        """根据训练数据集x_train, y_train训练Simple Linear Regression模型"""
        assert x_train.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert len(x_train) == len(y_train), \
            "the size of x_train must be equal to the size of y_train"

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)

        self.a_ = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)
        self.b_ = y_mean - self.a_ * x_mean

        return self

    def predict(self, x_predict):
        """给定待预测数据集x_predict，返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, \
            "must fit before predict!"

        return np.array([self._predict(x) for x in x_predict])

    def _predict(self, x_single):
        """给定单个待预测数据x，返回x的预测结果值"""
        return self.a_ * x_single + self.b_

    def score(self, x_test, y_test):
        """根据测试数据集 x_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(x_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "SimpleLinearRegression()"

```

## 多元线性回归

### 正规方程解

假设找到了最佳拟合的直线方程
$$
y = \theta_0 + \theta_1X_1 + \theta_2X + \cdots + \theta_nX_n
$$
对于每一个样本点$x^{(i)}$，根据直线方程，预测值为
$$
\hat{y}^{(i)} = \theta_0 + \theta_1X_1^{(i)} + \theta_2X_2^{(i)} + \cdots + \theta_nX_n^{(i)}
$$
可转换为
$$
\theta = (\theta_0,\theta_1,\theta_2\cdots \theta_n)^T
$$

$$
X^{(i)} = (X_0^{(i)},X_1^{(i)},X_2^{(i)},\cdots,X_n^{(i)}), X_0^{(i)}\equiv1
$$

$$
\hat{y}^{(i)} = X^{(i)}\cdot\theta
$$

可得到
$$
X_b = \left( \begin{array}{ccc} 1 & X_1^{(1)} & X_2^{(1)} & \ldots & X_n^{(1)} \\ 1 &  X_1^{(2)} & x_2^{(2)} & \ldots & X_n^{(2)} \\ \cdots &&&& \cdots \\ 1 & X_1^{(m)} & X_2^{(m)} &\ldots & X_n^{(m)}\end{array} \right)
$$

$$
\theta = \left( \begin{array}{ccc} \theta_0 \\ \theta_1 \\ \theta_2 \\ \ldots\\ \theta_n \end{array} \right)
$$

$$
\hat{y} = X_b\cdot\theta
$$

则损失函数为
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}=(y-X_b\cdot\theta)^T(y-X_b\cdot\theta)=J(\theta)
$$
求$J(a,b)$函数的最小值，采用求导
$$
\frac{\partial{J(\theta)}}{\partial{\theta}}
$$
求得$\theta$为(正规方程解)
$$
\theta = (X_b^TX_b)^{-1}X_b^Ty
$$
时间复杂度过高:O(n^3)，优化后也有O(n^2.4)

无量纲，故不需归一化

####  实现

类创建

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None  # 系数
        self.intercept_ = None  # 截距
        self._theta = None

    def fit_normal(self, X_train, y_train):
        """根据训练数据集X_train, y_train训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"
```

使用

```python
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

reg = LinearRegression()
reg.fit_noraml(X_train, y_train)
reg.score(X_test, y_test)
print(reg.coef_, reg.interception_)
```

#### sklearn

```python
from sklearn.linear_model import LinearRegression
 
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_reg.score(X_test, y_test)
print(lin_reg.intercept_, lin_reg.coef_)
print(np.argsort(lin_reg.coef_))
```

### 批量梯度下降

优缺点

```
注意：数据需要归一化，否则可能不收敛
优点： 对于多维度数据，梯度下降比正规方程速度更快
缺点：由于要计算所有样本进行计算，大批量样本速度会慢
```
#### 原理
$$
-\eta\nabla{J}
$$

其中$\nabla{J}$为
$$
\nabla{J} = (\frac{\partial{J}}{\partial{\theta_0}},\frac{\partial{J}}{\partial{\theta_1}},\ldots,\frac{\partial{J}}{\partial{\theta_n}})
$$
应用至损失函数
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2} = \sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}
$$
使线性回归中的损失函数更改为
$$
\frac{1}{m}\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\frac{1}{m}\sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}=J(\theta) = MSE(y, \hat{y})
$$

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

#### 实现

- 循环

模拟梯度下降

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ(theta, X_b, y):
  	res = enp.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
      	res[i] = (X_b.dot(theta) - y).doct(X-b[:,i])
    return res * 2 / len(X_b)
  
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta[0], theta[1:])
```

类创建

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            res = np.empty(len(theta))
            res[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            return res * 2 / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"

```

使用

```python
# 数据集
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

lin_reg = LinearRegression()
lin_reg.fit_gd(X, y)
lin_req.coef_
lin_req.intercept_
```

- 向量化

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

转换为
$$
\frac{2}{m}\cdot(X_b^{(1)}\theta-y^{(1)},X_b^{(1)}\theta-y^{(1)},\ldots,X_b^{(m)}\theta-y^{(m)})\cdot\left( \begin{array}{ccc} X_0^{(1)} & X_1^{(1)} & \ldots & X_n^{(1)} \\ X_0^{(2)} & X_1^{(2)} & \ldots & X_n^{(2)} \\ \ldots\\ X_0^{(m)} & X_1^{(m)} & \ldots & X_n^{(m)} \end{array} \right) = \frac{2}{m}\cdot(X_b\theta-y)^T\cdot{X_b}
$$
行列转换
$$
\nabla{J(\theta)}=\frac{2}{m}\cdot{X_b^T}\cdot{(X_b\theta-y)}
$$


实现

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            # res = np.empty(len(theta))
            # res[0] = np.sum(X_b.dot(theta) - y)
            # for i in range(1, len(theta)):
            #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            # return res * 2 / len(X_b)
            return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"
```

使用

```python
# 数据集
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

standardScaler = StandardSclar()
standarScaler.fit(X_train)
X_train_standard = standardScalar.transform(X_train)

lin_reg = LinearRegression()
lin_reg.fit_gd(X_train_standard, y_train)
lin_req.coef_
lin_req.intercept_
X_test_standard = standardScalar.transform(X_test)
lin_req.score(X_test_standard, y_test)
```

### 随机梯度下降

```
Stochastic Gradient Descent
特点：
对于样本数量过多，使用梯度下降可能会较慢，使用随机梯度下降进行优化
具有一定不可预知性，并不是一直在下降，可能有部分上升
eta需要随着循环次数增加而逐渐减小
可能跳出局部最优解
```

#### 原理

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

可以选择一项作为搜索方向
$$
2\cdot\left( \begin{array}{ccc} {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)=2\cdot(X_b^{(i)})^T\cdot(X_b^{(i)}-y^{(i)})
$$
对于$\eta $的值
$$
\eta = \frac{a}{iters+b}
$$
#### 实现

- 自定义

```python
import numpy as np
import matplotlib.pyplot as plt

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.
  
def sgd(X_b, y, initial_theta, n_iters):
  	t0 = 5
    t1 = 50
    def learning_rate(t):
      	return t0 / (t + t1)
    theta = initial_theta
    for cur_iter in range(n_iters):
      	rand_i = np.random.randint(len(X_b))
        grdient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * grdient
    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)
print(theta[0], theta[1:])
```

- 类创建

```python
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        assert n_iters >= 1

        def dJ_sgd(theta, X_b_i, y_i):
            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.

        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            def learning_rate(t):
                return t0 / (t + t1)

            theta = initial_theta
            m = len(X_b)

            for cur_iter in range(n_iters):
                indexes = np.random.permutation(m)
                X_b_new = X_b[indexes]
                y_new = y[indexes]
                for i in range(m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    theta = theta - learning_rate(cur_iter * m + i) * gradient

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.random.randn(X_b.shape[1])
        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"

```

使用

```python
# 模拟
import numpy as np
import matplotlib.pyplot as plt
from playML.LinearRegression import LinearRegression

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

lin_req = LinearRegression()
lin_req.fit_sgd(X, y, n_iters=2)
print(lin_req.coef_)
print(lin_req.intercept_)

# 数据集
from sklearn import datasets
from playML.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_test = train_test_split(X, y, sed=666)

standardScaler = StandardScalar()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

lin_req = LinearRegression()
lin_req.fit_sgd(X_train_standard, y_train, n_ites=5)
lin_req.score(X_test_standard, y_test)
```

#### sklearn

实现更为复杂，效率更为优化，只能用于线性回归

```python
from sklearn.linear_model import SGDRegressor

sgd_req = SGDRegressor()
# sgd_req = SGDRegressor(n_iter=100)
sgd_req.fit(X_train_standard, y_train, n_ites=5)
sgd_req.score(X_test_standard, y_test)
```

### 小批量梯度



